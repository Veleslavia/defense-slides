<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Olga Slizovskaia PhD thesis defense 21 October 2020</title>


		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="css/theme/upf.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<aside class="notes">
						The complementary relationship between audio and video data drives multimodal studies across
						various domains and problems. Within that scope, we emphasise the important role that visual
						modality plays in music perception and conduct a study on multimodal data fusion techniques in
						the music information retrieval domain. In this talk, we outline challenges in audio-visual
						music information retrieval and present our work that addresses instrument classification and
						source separation problems in musical instrument performances. We discuss the efficiency of
						conditioning techniques being applied at different levels of a primary network and make use of
						two extra modalities of data, namely instrument labels and the corresponding visual stream data.
					</aside>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<img src="img/image52.jpg" class="logo" style="height: 50px">
					<img src="img/image56.png" class="logo" style="height: 50px">
					<img src="img/image54.jpg" class="logo" style="height: 50px">

					<h2>Audio-Visual Deep Learning Methods for <br> Musical Instrument Classification
						and Separation</h2>
					<p><em>PhD candidate:</em> Olga Slizovskaia </p>
					<div class="multiCol" style="margin-left: 5em">
						<div class="col" style="font-size: smaller">
							<p><em>Supervisors:</em></p>
							Dr. Emilia Gómez <h4>Joint Research Centre, EC <br> Music Technology Group, UPF</h4>
							<p></p>
							Dr. Gloria Haro <h4>Image Processing Group, UPF</h4>
						</div>
						<div class="col" style="font-size: smaller">
							<p><em>Committee:</em></p>
							Dr. Xavier Giró-i-Nieto <h4>Universitat Politècnica de Catalunya</h4>
							<p></p>
							Dr. Xavier Serra <h4>Universitat Pompeu Fabra</h4>
							<p></p>
							Dr. Estefanía Cano <h4>Agency for Science, Research and Technology, <br> A*STAR, Singapore</h4>
						</div>
					</div>
					<p><em>21/10/2020</em></p>
					<aside class="notes">
						Dear commitee, my supervisors and everyone. I'm proud to be here with you today and to present
						my PhD thesis titled "Audio-Visual Deep Learning Methods for Musical Instrument Classification
						and Separation".

						At first, I will talk about what is audio-visual deep learning, what are the task which we are
						interested in, why you should care about multimodal methods and multimodal deep learning at all
						I'll explain why it is not so straightforward as it may look and present some of my work.
					</aside>
				</section>
				<section style="text-align: left">
					<h3>Outline</h3>
					<h2>I - Audio-Visual MIR</h2>
					<h2>II - Audio-Visual Musical Instrument Classification </h2>
					<h2>III - Audio-Visual Source Separation </h2>
					<h2>IV - Conclusion</h2>
				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>I - Audio-Visual MIR</h2>
				</section>

				<section>
					<aside class="notes">
						First of all, audio-visual MIR.

						When I was 11 y.o.,
						I somehow passed through a regional selection stage and
						my music teacher brought me to compete to a prestigious-like
						 musical competition in my area.

						I did not proceed further but I remember the experience of whatching
						(watching, not listening) performances of other participant,
						especially the senior ones.

						It was a feast of performance techniques, both from a technical and artistic
						point of view, and I recall that we discussed for a couple of weeks everything
						that we saw and tried to adapt some tricks.

						That was my first immersive experience of how highly multimodal could be our perception
						of a music performance.

						But definitely not the last one.

						So, what is audio-visual learning and why do we study it?
						First of all, we should care about AV methods because neither machine perception nor human
						perception is perfect and  while machine perception is trying to mimic human perception sometimes with success,
						many problems remain open, especially for complex scenes that involve several perception channels.
						Music one such phenomenon, it's multimodal by nature from real-life performance to modern music videos.
						And once we try to analize it, we focus on the following tasks:

					</aside>
						<h3>Why do we study Audio-Visual MIR?</h3>
						<ul>
							<li class="fragment">Music is multimodal, we aim for better understanding and analysis</li>
							<li class="fragment">Humans are very good at merging different sources of information</li>
							<li class="fragment">Yet, ML algorithms are not as good as humans</li>
							<li class="fragment">Various practical applications (alignment, transcription, separation, localization, tagging etc.)</li>
						</ul>
				</section>

				<section>
					<aside class="notes">
					</aside>
						<h3>What are the challenges in Audio-Visual MIR?</h3>
						<ul>
							<li class="fragment">Shortage of dedicated datasets</li>
							<li class="fragment">Low quality of available data, large diversity in data</li>
							<li class="fragment">Dimensionality mismatch problem</li>
							<li class="fragment">Data aggregation problem</li>
						</ul>
				</section>

				<section>
					<aside class="notes">
						This question is unseparable from the idea of data representation and  learned representation
					</aside>
					<section data-transition="none">
						<h3>Research problem I</h3>
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<br>
						<h3> Early fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/earlyfusion.png">
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<br>
						<h3> Late fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/latefusion.png">
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<br>
						<h3> Hybrid fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/hybridfusion.png">
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
				</section>

				<section>
						<section data-transition="none">
						<h3>Research problem II</h3>
						<ul>
							<li class="question" style="margin-bottom: 1em">How should we merge data from different sources?</li>
						</ul>
						</section>
						<section data-transition="none">
							<h3>Concatenation (additive conditioning)</h3>
							<figure class="l-body">
    							<svg viewBox="0 0 704 230" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path><path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path><path id="column-top" d="M 0 6 a 6 6 0 0 1 6 -6 l 8 0 a 6 6 0 0 1 6 6 l 0 14 l -20 0 Z"></path><path id="column-middle" d="M 0 0 l 20 0 l 0 20 l -20 0 Z"></path><path id="column-bottom" d="M 0 0 l 20 0 l 0 14 a 6 6 0 0 1 -6 6 l -8 0 a 6 6 0 0 1 -6 -6 Z"></path></defs><text x="260" y="10" dy="1em" class="figure-text"><tspan><tspan style="font-weight: bold;">Concatenation-based conditioning</tspan></tspan><tspan x="260" dy="1.5em"> simply concatenates the conditioning </tspan><tspan x="260" dy="1.5em"> representation to the input. </tspan></text><text x="445" y="100" dy="0.4em" class="figure-text"><tspan> The result is passed </tspan><tspan x="445" dy="1.5em"> through a linear layer </tspan><tspan x="445" dy="1.5em"> to produce the output. </tspan></text><g transform="translate(10, 70)"><text x="0" y="90" dy="0.4em" class="figure-text"> input </text><g transform="translate(40, 60)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use></g><text x="191" y="-30" dy="-0.35em" style="text-anchor: end;" class="figure-text"><tspan>conditioning</tspan><tspan x="191" dy="1.5em">representation</tspan></text><g transform="translate(201, -60)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-film-generator figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 40)" class="figure-film-generator figure-element"></use></g><g transform="translate(191, 30)"><rect width="40" height="120" class="figure-element figure-box"></rect><text x="20" y="60" dy="0.4em" style="text-anchor: middle;" transform="rotate(-90, 20, 60)" class="figure-text">concatenate</text></g><g transform="translate(302, 30)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 80)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 100)" class="figure-filmed-network figure-element"></use></g><g transform="translate(380, 30)"><rect width="40" height="120" class="figure-element figure-box"></rect><text x="20" y="60" dy="0.4em" style="text-anchor: middle;" transform="rotate(-90, 20, 60)" class="figure-text">linear</text></g><g transform="translate(570, 50)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use></g><text x="600" y="90" dy="0.4em" class="figure-text"> output </text><path d="M 60 90 L 186 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(186, 90)" class="figure-path"></use><path d="M 231 90 L 297 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(297, 90)" class="figure-path"></use><path d="M 327 90 L 375 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(375, 90)" class="figure-path"></use><path d="M 420 90 L 565 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(565, 90)" class="figure-path"></use><path d="M 211 0 L 211 25" class="figure-line"></path><use xlink:href="#arrow-down" transform="translate(211, 25)" class="figure-path"></use></g></svg>
  							</figure>
							<div class="remark">
								* Image courtesy: <a href="https://distill.pub/2018/feature-wise-transformations/">Dumoulin, et al., "Feature-wise transformations", Distill, 2018. </a></li>
							</div>
						</section>
						<section data-transition="none">
							<h3>Multiplicative conditioning</h3>
							<figure class="l-body">
								<svg viewBox="0 0 704 300" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path><path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path><path id="column-top" d="M 0 6 a 6 6 0 0 1 6 -6 l 8 0 a 6 6 0 0 1 6 6 l 0 14 l -20 0 Z"></path><path id="column-middle" d="M 0 0 l 20 0 l 0 20 l -20 0 Z"></path><path id="column-bottom" d="M 0 0 l 20 0 l 0 14 a 6 6 0 0 1 -6 6 l -8 0 a 6 6 0 0 1 -6 -6 Z"></path></defs><text x="340" y="30" dy="1em" class="figure-text"><tspan><tspan style="font-weight: bold;">Conditional scaling</tspan> first maps the </tspan><tspan x="340" dy="1.5em"><tspan style="font-weight: bold;">conditioning representation</tspan> to a </tspan><tspan x="340" dy="1.5em"> scaling vector. </tspan></text><text x="340" y="200" dy="1em" class="figure-text"><tspan> The scaling vector is then multiplied </tspan><tspan x="340" dy="1.5em"> with the input. </tspan></text><g transform="translate(-10, 10)"><text x="20" y="250" dy="-0.5em" style="text-anchor: begin;" class="figure-text">input</text><text x="605" y="250" dy="-0.5em" style="text-anchor: begin;" class="figure-text">output</text><g transform="translate(60, 200)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use></g><path d="M 80 240 L 300 240" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(300, 240)" class="figure-path"></use><g transform="translate(315, 240)"><circle r="10" class="figure-element"></circle><ellipse rx="2" ry="2" class="figure-path"></ellipse></g><path d="M 325 240 L 570 240" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(570, 240)" class="figure-path"></use><g transform="translate(575, 200)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use></g><text x="20" y="60" dy="-0.5em" style="font-weight: bold;" class="figure-text"><tspan>conditioning</tspan><tspan x="20" dy="1.5em">representation</tspan></text><g transform="translate(170, 0)"><rect width="40" height="120" class="figure-film-generator figure-element figure-box"></rect><text x="20" y="60" dy="0.4em" style="text-anchor: middle;" transform="rotate(-90, 20, 60)" class="figure-text">linear</text></g><g transform="translate(305, 20)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-film-generator figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-film-generator figure-element"></use></g><path d="M 120 60 L 165 60" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(165, 60)" class="figure-path"></use><path d="M 210 60 L 300 60" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(300, 60)" class="figure-path"></use><path d="M 315 100 L 315 225" class="figure-line"></path><use xlink:href="#arrow-down" transform="translate(315, 225)" class="figure-path"></use></g></svg>
							</figure>
							<div class="remark">
								* Image courtesy: <a href="https://distill.pub/2018/feature-wise-transformations/">Dumoulin, et al., "Feature-wise transformations", Distill, 2018. </a></li>
							</div>
						</section>

						<section data-transition="none">
							<h3>Feature-wise Linear Modulation</h3>
							<figure class="l-body">
								<svg viewBox="0 0 704 510" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path><path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path></defs><text x="0" y="20" class="figure-text"><tspan> The <tspan style="font-weight: bold;">FiLM generator</tspan> processes the conditioning information </tspan><tspan x="0" dy="1.5em"> and produces parameters that describe how the target network </tspan><tspan x="0" dy="1.5em"> should alter its computation. </tspan></text><path d="M420,10l0,510" style="stroke-width: 1px; stroke: #666; opacity: 0.15;"></path><text x="460" y="20" class="figure-text"><tspan> Here, the <tspan style="font-weight: bold;">FiLM-ed network</tspan>’s computation </tspan><tspan x="460" dy="1.5em"> is conditioned by two FiLM layers. </tspan></text><g transform="translate(0, 40)"><g transform="translate(460, 40)"><text x="150" y="425" class="figure-text">output</text><path d="M130,399L130,430" class="figure-line"></path><use x="130" y="430" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 337)" class="figure-faded"><rect width="222" height="62" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="31" dy="0.4em" style="text-anchor: middle;" class="figure-text">sub-network</text></g><path d="M130,305.5L130,332" class="figure-line"></path><use x="130" y="332" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 275.5)" class="film-layer"><rect width="222" height="30" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="15" dy="0.4em" style="text-anchor: middle; font-weight: bold;" class="figure-text">FiLM</text></g><path d="M130,244L130,270.5" class="figure-line"></path><use x="130" y="270.5" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 182)" class="figure-faded"><rect width="222" height="62" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="31" dy="0.4em" style="text-anchor: middle;" class="figure-text">sub-network</text></g><path d="M130,149.5L130,177" class="figure-line"></path><use x="130" y="177" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 119.5)" class="film-layer"><rect width="222" height="30" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="15" dy="0.4em" style="text-anchor: middle; font-weight: bold;" class="figure-text">FiLM</text></g><path d="M130,88L130,114.5" class="figure-line"></path><use x="130" y="114.5" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 26)" class="figure-faded"><rect width="222" height="62" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="31" dy="0.4em" style="text-anchor: middle;" class="figure-text">sub-network</text></g><text x="150" y="0" class="figure-text">input</text><path d="M130,-15l0,36" class="figure-line"></path><use x="130" y="21" xlink:href="#arrow-down" class="figure-path"></use></g><g transform="translate(0, 190)"><path d="M0,0l110,0" class="figure-line"></path><use x="110" y="0" xlink:href="#arrow-right" class="figure-path"></use><text x="0" y="0" dy="-0.8em" class="figure-text">conditioning</text></g><g transform="translate(115, 140)" class="film-generator"><rect width="180" height="100" class="figure-film-generator figure-group figure-box"></rect><text x="90" y="50" dy="0.4em" style="text-anchor: middle; font-weight: bold;" class="figure-text">FiLM generator</text></g><g><path d="M295,175l175,0" class="figure-line"></path><use x="475" y="175" xlink:href="#arrow-right" class="figure-path"></use><text x="305" y="175" dy="-0.8em" class="figure-text">FiLM parameters</text></g><g><path d="M295,205C360,205,387.5,205,387.5,265S415,330,475,330" class="figure-line"></path><use x="475" y="330" xlink:href="#arrow-right" class="figure-path"></use></g></g></svg>
							</figure>
							<div class="remark">
								* Image courtesy: <a href="https://distill.pub/2018/feature-wise-transformations/">Dumoulin, et al., "Feature-wise transformations", Distill, 2018. </a></li>
							</div>
						</section>

				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>II - Audio-Visual Musical Instrument Classification</h2>
				</section>

				<section>
					<aside class="notes">
						Video source: https://www.youtube.com/watch?v=aaxX9Ik6he0 CC0
					</aside>
					<h3> Problem definition </h3>

					<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/problem_definition_informal.png">
				</section>

				<section>
					<aside class="notes">
						For the classification setup, we have a set of objects $X \in \mathbb{R}^N$ and a set of class
						labels $C \in \{0,1\}^K$. We are interested in predicting a final subset of
						labels $y=(y_1, y_2, ..., y_K) \in C$ for a sample $x=(x_1, x_2, ..., x_N) \in X$ which is
						an $N$-dimensional representation of an object $x$. Therefore, we are looking for a function such as

						where $\theta$ are the function parameters and $ \hat{y} $ is a probability vector for the
						class estimates. A common mapping function $ f $ could be a complex non-linear neural
						network, where the final probability scores can be obtained via softmax function
						\[ \hat{y_j}(z) = \frac{e^{z_j}}{\sum_{i=1}^L e^{z_i}}, \] which takes a vector of
						arbitrary real-valued scores $z = (z_1, ... z_L) \in \mathbb{R}^L $ and transforms
						it to a vector of values between zero and one that sum to one.

						A common choice to learn the parameters $\theta$ is through a backpropagation
						algorithm where one of the common  optimization techniques can be used to
						minimize a loss function. For multi-labels classification,
						the loss function can be defined as categorical cross-entropy between ground truth and
						estimated probability distributions of class labels:
					</aside>
					<h3> Formal problem definition </h3>

					<ul>
						<li class="fragment"> set of objects $ \definecolor{signal}{RGB}{18,110,213} {\color{signal} X} \in \mathbb{R}^N$,
							samples $ {\definecolor{signal}{RGB}{18,110,213} \color{signal}x}=({\color{signal} x_1, x_2, ..., x_N}) \in {\color{signal}X}$ </li>
						<li class="fragment"> set of class labels $\definecolor{categories}{RGB}{203,23,206} {\color{categories}C} \in \{0,1\}^{\color{categories}K}$ </li>
						<li class="fragment"> final subset of labels $ \definecolor{classes}{RGB}{114,0,172}  \definecolor{categories}{RGB}{203,23,206}
							{\color{classes} y } =
							({\color{classes}y_1, y_2, ..., y_K}) \in {\color{categories} C }$ for a sample
							$ \definecolor{signal}{RGB}{18,110,213} \color{signal} x$ </li>
					</ul>
					<p class="fragment">
					\[
					\begin{aligned}
					\definecolor{classes}{RGB}{114,0,172}
					\definecolor{params}{RGB}{45,177,93}
					\definecolor{model}{RGB}{251,0,29}
					\definecolor{signal}{RGB}{18,110,213}
					\definecolor{probability}{RGB}{217,86,16}
					\definecolor{categories}{RGB}{203,23,206}

					{\color{probability} \hat{y}} = {\color{model} f}_{\color{params} \theta}({\color{signal} x})
					\end{aligned}
					\]

						<span style="color: rgb(45,177,93)"> find parameters</span>
						<span style="color: rgb(251,0,29)"> for a model </span> that gives <br>
						<span style="color: rgb(203,23,206)">class </span>
						<span style="color: rgb(217,86,16)">estimation probabilities </span>
						<span style="color: rgb(18,110,213)">for a sample</span>

					</p>

					<p class="fragment">
						\[

						\definecolor{loss}{RGB}{128,121,14}
						\definecolor{classes}{RGB}{114,0,172}
						\definecolor{probability}{RGB}{217,86,16}
						\definecolor{categories}{RGB}{203,23,206}

							{ \color{loss} \mathcal{L} } ( {\color{classes} y}, {\color{probability} \hat{y}})
						= -{\color{loss} \sum}_{\color{categories} i=1}^{\color{categories} K} {\color{classes}y_i} {\color{loss} * \log}( {\color{probability} \hat{y}_i })
						\]

						<span style="color: rgb(203,23,206)">for multi-labels classification, </span>
						minimize <span style="color: rgb(203,23,206)"> categorical </span>
						<span style="color: rgb(128,121,14)"> cross-entropy between </span>
						<span style="color: rgb(114,0,172)"> ground truth </span>
						<span style="color: rgb(128,121,14)"> and </span>
						<span style="color: rgb(217,86,16)"> estimated probability distributions </span>
						<span style="color: rgb(203,23,206)"> of class labels </span>
					</p>


				</section>

				<section>
					<h3> First multimodal approach </h3>
					<ul>
						<li class="fragment"> Hybrid/Late fusion</li>
						<li class="fragment"> Concatenation</li>
					</ul>
					<img class="fragment" height="300" style="margin-left: 2em; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/icmr_multimodal.png">

				</section>


				<section data-transition="none">
					<h3> First multimodal approach </h3>
					<img height="200" style="margin-left: 2em; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/icmr_multimodal.png">

					<div class="multiCol">
						<div class="col fragment">
						<ul>
							<li> MelSpec audio representation </li>
							<li> Architectures
								<ul style="list-style:none">
									<li> Han et al., 1.5M params</li>
									<li> Choi et al., 2.4M params</li>
									<li> Xception, 9.6M params</li>
								</ul>
							</li>
						</ul>
						</div>

						<div class="col fragment">
						<ul>
							<li> Bag-of-frames RGB</li>
							<li> Number of frames: 10-100
								<ul style="list-style:none">
									<li> FCVID: 20-100 </li>
									<li> YouTube-8M: 10-20 </li>
								</ul>
							</li>
							<li>Fine-tune or train from scratch</li>
						</ul>
						</div>
					</div>

					<blockquote class="fragment">
						DATASETS
						<ul>
							<li> FCVID: Musical Performance With Instruments <br> &emsp; 12 classes, 5K videos, 260 hours </li>
							<li> YouTube-8M: MusInstr-Normalized <br> &emsp; 46 classes, 60k videos, 4k hours </li>
						</ul>
					</blockquote>

				</section>

				<section>
					<section data-transition="none" data-markdown>
						<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | `$ \mathbf{88.27}$` | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | `$87.25$` | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | `$78.95$` | `$+5.21$` | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | `$ \mathbf{84.69}$`| `$-0.43$` | `$-13.6$` |

							 > <!-- .element: class="fragment" --> comparing Xception and Choi et al. as they perform compatible as audio-only models

						</textarea>

					</section>

					<section data-transition="none" data-markdown>
						<aside class="notes">Choi is smaller and have stronger regularization, stronger features </aside>

						<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | `$ \mathbf{88.27}$` | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | `$87.25$` | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | `$78.95$` | <span style="background-color: #c8102e; border-radius: 5px"> &nbsp;`$+5.21$` </span> | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | `$ \mathbf{84.69}$`| `$-0.43$` | `$-13.6$` |

							<span style="background-color: #c8102e; border-radius: 5px"> &nbsp; `${}^*$` Issues with YouTube-8M annotations &nbsp; </span> <!-- .element: class="fragment" -->

							 > <!-- .element: class="fragment" --> TAKE AWAY:
							 > multi-modal fusion is beneficial overall <br>
							 > concatenation is not enough for controversial inputs

						</textarea>

					</section>

					<section data-markdown>
						<textarea data-template>
							### Audio-only results
							|Method | Params | Dataset | Hit@1 | Hit@3 | F1 |
							| :----- | -------- | --- | ---- | ----- | --- |
							| Han et al. | 1.5M |  FCVID  |  64.13 | 76.82 | 53.64 |
							| Choi et al. + CC | 2.4M |  FCVID |  77.73 | 92.05 | 77.18 |
							| Choi et al. + UC | 2.4M |  FCVID |  **79.81** | **96.09** | 78.71 |
							| Xception + UC | 9.6M |  FCVID | 78.69 | 94.44 | **79.35** |
							| Han et al. | 1.5M | YT-8M | 59.37 | 70.87 | 56.50 |
							| Choi et al. + UC | 2.4M | YT-8M | **83.58** | 94.23 | **84.26** |
							| Xception + UC | 9.6M | YT-8M | 83.53 | **94.69** | 84.16 |
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### Visual-only results

							|Dataset | FMs | PT | Steps | Time | Hit@1 | Hit@3 | F1 |
							| :----- | --- | --- | --- | ---- | ----- | ------ | --- |
							|FCVID  | 20 | No | 32K | 19h | 42.30 | 64.53 | 43.16 |
							|FCVID  | 30 | No | 16K | 11h | 65.39 | 81.75 | 67.29 |
							|FCVID  | 30 | Yes | 16K | 11h | 68.77 | 84.26 | 70.33 |
							|FCVID  | 50 | No | 24K | 22h | 67.47 | 83.21 | 69.38 |
							|FCVID  | 50 | Yes | 21K | 19h | **69.39** | **84.32** | **71.23** |
							|FCVID  | 100 | No | 43K | 98h | 68.56 | 83.97 | 70.42 |
							|FCVID  | 100 | Yes | 36K | 84h | 67.76 | 83.50 | 69.16 |
							|YT-8M  | 10 | No | 58K | 82h | 61.15 | 78.45 | 52.19 |
							|YT-8M  | 20 | Yes | 57K | 92h | **70.07** | **84.20** | **71.09** |
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Audio-visual results
							| Method | Dataset | Hit@1 | Hit@3 | F1 |
							| :----- | -------- | --- | ---- | ----- |
							|Xception @ 50 frames | FCVID | **88.28** | **97.00** | **88.27** |
							|Choi et al. @ 50 frames | FCVID | 86.97 | 96.09 | 87.25 | 
							|Xception @ 20 frames | YT-8M | 82.64 | 91.37 | 78.95 |
							|Choi et al. @ 20 frames | YT-8M | **84.01** | **93.41** | **84.69** |
						</textarea>
					</section>

				</section>


				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>III - Audio-Visual Source Separation</h2>
				</section>

				<section>
					<h2>Can you hear the difference?</h2>
					<br>
					<p>&nbsp;</p>
					<p>&nbsp;</p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_1.wav" > <span class="music">Sample 1</span> </p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_2.wav" > <span class="music">Sample 2</span></p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_3.wav" > <span class="music">Sample 3</span></p>
					<p>&nbsp;</p>
					<p>&nbsp;</p>
					<div class="remark">
						* All examples are taken from the URMP dataset: <a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a></li>
					</div>

					<aside class="notes">
						To introduce the topic I'm going to let you hear three short excerpts of the same composition
						(the same musical piece) and ask you a simple question.
						I believe that most you can hear a significant difference between sample 3 and the first two
						examples but... what it's much harder in case of sample 1 and sample 2.
						how many could hear the difference? The overlap is so strong that it's almoust impossible without
						special training.
						Now, I would like you to hear the actual difference between the three of them.
					</aside>
				</section>

				<section>
					<p>Can you hear the difference <span style="color: orange">now?</span></p>
					<br>

					<aside class="notes"> 
						This is how the first mixture is different from the second one.
						This is how the second mixture is different from the first one.
						This is residue of the third mixture with respect to the first two.

						Can you say if the first samples are from the same musical instrument or not?
						Would it be easier for you to distinguish between two recordings if I could provide you more information.... like this?
					</aside>

					<table>
						<thead>
							<tr>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_1_db.wav" >
									<span class="music">Separated 1</span>
								</span></th>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_2_vc.wav" >
									<span class="music">Separated 2</span>
								</span></th>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_3_cl.wav" >
									<span class="music"> Separated 3 </span>
								</span></th>
							</tr>
						</thead>
						<tbody>
							<tr class ="fragment">
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_1_db.gif"></td>
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_2_vc.gif"></td>
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_3_cl.gif"></td>
							</tr>
						</tbody>
					</table>

				</section>

				<section>
					<aside class="notes"> 
						Just keep it in mind for a little, while I'm continue to introduce the task which we're trying to solve.
						Having a single-channel recording of a musical performance, that is to say a mixture of many possibly overlapping signals, we want to estimate their individual tracks, their sources.
						It can be used for hearing aid, music production, music education, as a preprocessing for other analysis tasks and so on.
						Here is an illustration of our goal.  
					</aside>

						<h3> Problem definition </h3>
						<video controls>
							<source src="video/vimss_idea_demo.mp4" type="video/mp4">
						</video>
				</section>

				<section>
					<aside class="notes">
					Single channel source separation (SCSS) consists of estimating the individual sources
						x_i given a mono mixture time-domain signal $y$ of $N$ sources:

					Instead of predicting time-domain signals, a general approach for solving SCSS involves the
						estimation of N masks for Short-Term Fourier transform (STFT) values of the mixture.
						In this case, we consider a time-frequency representation of the mixture Y
						and the  sources Xi, and the goal of the source separation method is to
						learn a real-valued (or complex-valued) mask $M_i$ for each source $i$ .

					Let us denote by $|\boldsymbol{X_i}(\tau, \omega)|$ and $|\boldsymbol{Y}(\tau, \omega)|$
						the magnitude of the STFT value, of $\boldsymbol{X_i}$ and $\boldsymbol{Y}$ respectively,
						at frequency $\omega$ and time frame $\tau$.
					In this work, we only consider two types of \textit{real-valued} masks,
						namely \textit{ideal ratio or soft} masks $M_i^{ir}$:

					</aside>
					<section data-transition="none">
					<h3> Formal problem definition </h3>

					<ul>
						<li class="fragment"> a mono mixture time-domain signal $\definecolor{mix}{RGB}{18,110,213} \color{mix} y(t)$ </li>
						<li class="fragment"> individual sources $ \definecolor{gt}{RGB}{203,23,206}  \color{gt} x_i(t)$ </li>
					</ul>
					<p class="fragment">
							\[
								\definecolor{loss}{RGB}{128,121,14}
								\definecolor{predicted}{RGB}{217,86,16}
								\definecolor{gt}{RGB}{203,23,206}
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{sum}{RGB}{251,0,29}
								\definecolor{mix}{RGB}{18,110,213}

								{\color{mix} y(t)} = {\color{sum} \sum_{\color{nsources}i=1}^{\color{nsources}N} {\color{gt} x_i(t)}}

							\]

							<span style="color: rgb(18,110,213)"> the mixture </span>
							equals <span style="color: rgb(251,0,29)">to the sum of </span>
							<span style="color: rgb(114,0,172)"> all </span>
							<span style="color: rgb(203,23,206)"> sources </span>
					</p>
					<p class="fragment">
							\[
								\definecolor{loss}{RGB}{128,121,14}
								\definecolor{predicted}{RGB}{217,86,16}
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{model}{RGB}{251,0,29}
								\definecolor{gt}{RGB}{203,23,206}
								\definecolor{mix}{RGB}{18,110,213}
								\definecolor{param}{RGB}{45,177,93}

								{\color{predicted} \hat{x}_{\color{nsources}i}(t)} =
									{\color{model}f_{\color{params}\theta}^{\color{nsources}i}}({\color{mix}y(t)})

							\]

							<span style="color: rgb(45,177,93)"> find parameters</span>
							<span style="color: rgb(251,0,29)"> for a model </span> that gives <br>
							<span style="color: rgb(114,0,172)"> individual </span>
							<span style="color: rgb(217,86,16)"> sources estimation </span>
							<span style="color: rgb(18,110,213)">from the muxture</span>
						</p>
					</section>

					<section data-transition="none">
						<h3> Formal problem definition </h3>
						<p>approaches to estimate individual sources from the muxture</p>
						<div class="multiCol">

						<div class="col fragment" style="text-align: center">
							<h4> direct waveform estimation</h4>
							<p>predicting time-domain signals $
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{predicted}{RGB}{217,86,16}
								\color{predicted} \hat{x}_{\color{nsources}i}(t)$</p>
							<p class="fragment">
							\[
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{loss}{RGB}{128,121,14}
								\definecolor{predicted}{RGB}{217,86,16}
								\definecolor{gt}{RGB}{203,23,206}

							{\color{loss} \mathcal{L}^{w} {\color{black}=} \sum_{\color{nsources}i=1}^{\color{nsources}N}
								\sum_{j=1}^{T} ({\color{gt} x_{\color{nsources}i}(j)} -
								{\color{predicted} \hat{x}_{\color{nsources}i}(j)})^2} \]

							</p>
						</div>
						<div class="col fragment" style="text-align: center">
							<h4>masking-based approach</h4>
							<p>predicting ratio masks $
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{predicted}{RGB}{217,86,16}
								\color{predicted} \hat{M}_{\color{nsources}i}^{r} $</p>
							<p> <span style="font-size: medium"> $ \definecolor{mix}{RGB}{18,110,213} \color{mix}
								\boldsymbol{Y} $,
								$ \definecolor{gt}{RGB}{203,23,206}
								  \definecolor{nsources}{RGB}{114,0,172} \color{gt}
									\boldsymbol{X_{\color{nsources} i}} $
									STFT values of the mixture and sources </span> </p>

							<p>	<span style="font-size: medium"> magnitude of the STFT value at frequency $\nu$ and time $\tau$ <br>
								$\definecolor{gt}{RGB}{203,23,206}
								 \definecolor{nsources}{RGB}{114,0,172}
								| {\color{gt} \boldsymbol{X_{\color{nsources}i}}(\tau, \nu) } |$,
								$\definecolor{mix}{RGB}{18,110,213}
								| {\color{mix} \boldsymbol{Y}(\tau, \nu)}|$ </span>
							</p>
							<p>
								<span style="font-size: medium"> ideal ratio mask of source $\definecolor{gt}{RGB}{203,23,206}
								 \definecolor{nsources}{RGB}{114,0,172}
								| {\color{gt} \boldsymbol{X_{\color{nsources}i}}(\tau, \nu) } |$

								\[	\definecolor{gt}{RGB}{203,23,206}
									\definecolor{nsources}{RGB}{114,0,172}
									\definecolor{mix}{RGB}{18,110,213}
									\definecolor{predicted}{RGB}{217,86,16}

									{\color{predicted} \hat{M}_{\color{nsources}i}^{r}} =

									\frac{|{\color{gt} \boldsymbol{X_{\color{nsources}i}}(\tau, \nu) } |}{|{\color{mix} \boldsymbol{Y}(\tau, \nu)}|}
								\]</span>
							</p>

							<p class="fragment">
							\[

							\definecolor{loss}{RGB}{128,121,14}
							\definecolor{nsources}{RGB}{114,0,172}
							\definecolor{predicted}{RGB}{217,86,16}
							\definecolor{gt}{RGB}{203,23,206}

							{\color{loss} \mathcal{L}^{r} {\color{black}=} \sum_{\color{nsources}i=1}^{\color{nsources}N} \sum_{j=1}^{|(\tau, \nu)|}
								({\color{gt} M_{\color{nsources}i}^{ir}(j)} -
								{\color{predicted} \hat{M}_{\color{nsources}i}^{r}(j)})^2} \]

						</p>

						</div>
					</div>

					</section>
				</section>



				<section>
					<aside class="notes"> 
						Now I would like to shortly discuss the common approach to solve this problem and why it is difficult to solve.
						One of the problems is that sources often overlap and time and frequency. It's especially true for instruments playing in harmony and in unison, this is often the case for classical music. And this is also a problem for instruments of the same family, since they have very similar timbre as well. 
						
						It has been shown that the complexity of the task increases with the number of sources, and in our work we tackle the problem not of just two-channel source separation, but we address multiple instrument separation where actual number of sources is not known.

						For many years standard common approach for this task had consisted in estimating binary or ratio masks which could be applied to a magnitude of mixture spectrogram in order to obtain the magnitude spectrogram of the sources, and then recostruct the original sound using either the mixture phase of phase-reconstruction algorithm.
						Thus we lose some information which can be potentially usefull for source separation and this is our primary motivation for waveform domain source separation.

						On the other hand, with all this complexity and the first example I've shown you in mind, we want to answer another question:

						So we took week information, which is instrument labels information which can be available from another classifier network from taken from a different modality, a decided to use them to condition our model.  

					</aside>

					<section>
						<h2>Known challenges</h2>
						<ul>
							<li class="fragment">Same-family instruments are similar to one another</li>
							<li class="fragment">Unknown in advance number of sources</li>
							<li class="fragment">Complexity increases with the number of sources</li>
							<li class="fragment">Overlap in time and frequency between sources</li>
							<li class="fragment">Extra: inter-family mimicry for some instruments (clarinet vs. viola)</li>
						</ul>
						<br>
						<br>
						<p class="fragment" style="width: 80%; margin: auto; background-color: hsl(11,65%,70%); padding: 20px 0px 20px 10px; color:white">
							<span style="font-size:48px; float: left">💪</span>
							<span style="font-size:48px">Can we use extra information to improve separation? </span>
						</p>
					</section>
				</section>

				<section data-transition="none">
					<aside class="notes"> 
						Let me briefly explain the architecture.
						Overall, in deep learning based source separation, there is a huge variety of architecture design. 
						All that start from an encoder-decoder architecture (but STFT-based), when people started to use U-net with skip connections but also STFT-based,
						then Wave-U-Net appeared, and we inhereted Wave-U-Net model from their authors. 
						But, first of all, we are trying to estimated multiple sources which are very sparce, and, finally,
						we apply a feature modulation with instrument labels on the bottleneck of the U-Net.

						It's multiplicative conditioning, because back then the authors weren't aware of FiLM. We only applied it to the bottleneck although later we tried other combinations.   
					</aside>

					<section data-transition="none">
						<h3>Preliminary study</h3>
						<ul>
							<li class="fragment">Multi-instrument Source Separation in Time Domain</li>
							<li class="fragment">Hybrid fusion</li>
							<li class="fragment">Multiplicative Conditioning on Instrument Labels</li>
						</ul>
					</section>


					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
							<img class="fragment" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net1.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net2.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net3.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net4.png">
					</section>
				</section>

				<section> 
					<aside class="notes"> 
						This was trained with a simple MSE loss between original and reconstructed sources, it was trained on a multi-modal dataset of music performances from Rochester university, which is relatively small and has its issues. 
						So, just a disclaimer, the results which I'm going to show you now are results of the network which has been trained with 30 videos which is about 1,5h in total.
					</aside>
					<section>
						<h3> Experiments </h3>
					</section>

					<section data-background-image="img/creation_process.png" data-state="no-title-footer" data-background-opacity="0.2" data-background-size="contain">
						<h4 class="fragment" data-fragment-index="1">URMP <a href="https://doi.org/10.5061/dryad.ng3r749">Dataset</a></h4>
						<ul class="fragment" data-fragment-index="2" style="background-color: rgba(255, 255, 255, 0.7)">
							<li class="fragment" data-fragment-index="2">13 instruments</li>
							<li class="fragment">44 videos -> 40 valid pieces </li>
							<li class="fragment">12 duest, 20 trios, 8 quartets </li>
							<li class="fragment">87 unique audio tracks </li>
							<li class="fragment">Train/Test split: 30/10 pieces </li>
						</ul>

					</section>

					<section>
						<h3>Video Demo: Trained on 30 videos</h3>
						<video controls>
							<source src="video/urmp_demo_multisource.mp4" type="video/mp4">
						</video>
					</section>

					<section>
						<aside class="notes"> 
							We compared our method with InformedNMF approch, which was the closest method able to separate sounds of different instruments and it uses pretrained timbre models. And, surprisingly, our method performs better in SIR.
						</aside>

						<h3>Summary of results</h3>
						<table>
							<thead>
								<tr>
									<th>Method</th>
									<th>SDR</th>
									<th>SIR</th>
									<th>SAR</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>InformedNMF</td>
									<th>-0.16</th>
									<th style="font-weight: normal;">1.42</th>
									<th style="font-weight: normal;">9.31</th>
								</tr>
								<tr>
									<td>Exp-Wave-U-Net</td>
									<th style="font-weight: normal;">-4.12</th>
									<th style="font-weight: normal;">-3.06</th>
									<th>12.18</th>
								</tr>
								<tr>
									<td>CExp-Wave-U-Net</td>
									<th style="font-weight: normal;">-1.37</th>
									<th>2.16</th>
									<th style="font-weight: normal;">6.36</th>
								</tr>
							</tbody>
						</table>
					</section>

				<section>

					<aside class="notes"> 
						But we achieve the most notable difference with the baseline when the complexity of the problem increases. 
						Here you can see that InformedNMF outperforms our model when we test task is the separation of two instruments, but as the number of sources increases, the conditioned wave-u-net notably outperforms the baseline.
					</aside>

					<h3>Summary of results</h3>
					<img data-src="img/sdr.png" height="300px" style="border:1px solid #555a5f">
					<img data-src="img/sir.png" height="300px" style="border:1px solid #555a5f">
					<!--<img data-src="img/sar.png" height="250px" style="border:1px solid #555a5f">-->
				</section>

				<section>

					<aside class="notes">
						Take away note: it was inspiring but we lacked TPUs and scaling was not an option
					</aside>

					<h3>Summary of results</h3>
					<blockquote>Take away note: it was inspiring but we lacked TPUs and scaling was not an option
					</blockquote>
				</section>

				<section>

					<aside class="notes"> 
						As I said, we took the original Wave-U-Net model and did quite a couple of changes. 
						For example, we optimized the learning rate, we optimized data loading pipeline, we ported it such that it was possible to train on GC TPUs (which we were able to use at the time), also tested half-precision, which resulted in 35 times faster training comparing to the original model. 
					</aside>

					<h3> Ablation and Speedup</h3>
					<p class="fragment">Learning rate</p>
					<p class="fragment">TPUs</p>
					<p class="fragment">tf.float32 vs tf.float16</p>
					<p class="fragment" style="font-size: larger; color: orange">Total Speedup: x35.4</p>
				</section>
			</section>

			<section>
				<p>What we did next...</p>

				<!--
				<p class="fragment">
					<a style="font-size: smaller;" href="https://veleslavia.github.io/conditioned-u-net/">https://veleslavia.github.io/conditioned-u-net/</a>
				</p>
				-->
			</section>

			<section data-background-image="img/spreadsheet.png" data-state="no-title-footer" data-background-opacity="0.2" data-background-size="cover">
				<h3 style="background-color: rgba(255, 255, 255, 0.7); margin-bottom: 0em;"> SOLOS </h3>
				<div class="remark" style="text-align: center; background-color: rgba(255, 255, 255, 0.7); margin-bottom: 2em;">
					* Joint work with Juan Montesinos <a href="https://juanmontesinos.com/Solos">https://juanmontesinos.com/Solos</a></li>
				</div>

				<ul>
					<li class="fragment" data-fragment-index="1" style="background-color: rgba(255, 255, 255, 0.7)">13 instruments matching the URMP dataset</li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 0.7)">Only solo performances / auditions </li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 0.7)">Semi-automatic and manual quality control </li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 0.7)">Mix-and-separate approach</li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 0.7)">755 individual recordings </li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 0.7)">Extra: valid timestamps and skeletons  </li>
				</ul>

				<p>&nbsp;</p>
				<p>&nbsp;</p>

			</section>

			<section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3 class="fragment" >Design Decisions</h3>
						<img class="fragment" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params.svg">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_baseline.svg">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_final.svg">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_final_data.svg">
				</section>

				<section data-transition="none">
					<h3>Context vectors</h3>
					<div class="multiCol">
						<div class="col fragment">
							<h5>Label conditioning</h5>
							<img data-src="img/cv_binary.png" height="300px" style="border:1px solid #555a5f">
						</div>
						<div class="col fragment">
							<h5>Visual conditioning</h5>
							<img data-src="img/cv_visual.png" height="300px" style="border:1px solid #555a5f">
						</div>
						<div class="col fragment">
							<h5>Visual-motion conditioning</h5>
							<img data-src="img/cv_motion.png" height="300px" style="border:1px solid #555a5f">
						</div>
					</div>
				</section>

				<section data-transition="none">
					<h3>Design Decisions</h3>
					<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_final_conditioning.svg">
				</section>

				<section data-transition="none">
					<h3>Conditioning Design</h3>
					<div class="multiCol">
						<div class="col fragment">
							<h5>FiLM conditioning</h5>
							<img height="300px" style="border:0; box-shadow: 0px 0px 0px #ffffff; " data-src="img/cunet_conditioning_film.png">
						</div>
						<div class="col fragment">
							<h5>Multiplicative conditioning</h5>
							<img height="300px" style="border:0; box-shadow: 0px 0px 0px #ffffff; " data-src="img/cunet_conditioning_multiply.png">
						</div>
					</div>

				</section>


			</section>

			<section>
				<section>
					<h3>Results: Video Demo</h3>
					<video controls>
						<source src="video/C-U-Net_Demo.mp4" type="video/mp4">
					</video>
				</section>

				<section>
					<h3>Summary of results</h3>

				</section>

				<section data-markdown style="font-size: medium">
					<textarea data-template>
					## Ablation study results

					|Method | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
					| :--- | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
					| IRM | U | `$ 13.1\pm5.4 $` | `$ 12.7\pm6.4 $`  | n/a | `$ 11.79\pm4.26 $` | `$ 19.94\pm5.59 $` | `$ 13.02\pm4.25 $` |
					| input mix | L | `$-3.7\pm5.7$` | `$-3.7\pm5.7$` | `$18.2\pm4.2$` | `$ -3.48\pm4.82 $` | `$ -3.20\pm4.95 $` | `$ 18.10\pm11.21 $` |
					| No filtering | 1 | `$  -2.1\pm 6.2 $` | `$ -15.2\pm 10.0 $` | `$ -8.1\pm 5.9 $` | `$ -0.77\pm5.76 $` | `$ 0.57\pm6.85 $` | `$ 10.88\pm3.21 $` |
					| Wiener (1 it.) | 1-W`$_1$` | `$ -3.9\pm 7.5 $` | `$ -16.0\pm 16.1 $` | `$ -15.1\pm 10.1 $` | `$ -1.01\pm6.20 $` | `$ 1.53\pm7.65 $` | `$ 7.70\pm3.81 $` |
					| Wiener (2 it.) | 1-W`$_2$` | `$ -7.6\pm 10.7 $` | `$ -21.4\pm 24.1 $` | `$ -25.4\pm 16.8 $` | `$ -2.56\pm7.10 $` | `$ \mathbf{1.72\pm8.52} $` | `$ 4.99\pm5.62 $` |
					| 1-W`$_2$` with CL | 2-W`$_2$` | `$ -8.2\pm 10.9 $` |  `$ -24.1\pm 25.1  $` |  `$ -26.2\pm 17.8  $` |  `$ -3.06\pm7.06  $` | <span style="color:gray">`$ 1.38\pm8.55 $`</span> | <span style="color:gray">`$ 4.59\pm5.72 $`</span>  |
					| 2-W`$_2$` but binary masks | 3-W`$_2$` |  <span style="color:gray">`$ -9.7\pm 15.2 $`</span> | <span style="color:gray">`$ -21.6\pm 27.4 $`</span> |  `$ \mathbf{-36.2\pm 18.4}  $` |  `$ -3.82\pm7.95  $` |  `$ 0.09\pm7.10  $` |  `$ 4.93\pm7.53  $`  |
					| 1-W`$_2$` with noise | 4-W`$_2$` |  <span style="color:gray">`$ -7.6\pm 9.2 $`</span> |  `$-21.7\pm 20.4  $` |  `$ -21.7\pm 15.6 $` |  `$ -3.12\pm6.91  $` |  `$ 0.21\pm8.34  $` |  `$ \mathbf{6.04\pm5.32} $` |
					| 1-W`$_2$` no dB normalise | 5-W`$_2$` | `$ -10.1\pm 11.6 $` |  `$ -25.3\pm 24.9  $` |  `$ -27.0\pm 17.0  $` |  `$ -3.88\pm7.63  $` |  `$ 0.70\pm9.45  $` |  `$ 4.63\pm6.13  $` |
					| 1-W`$_2$` linear-scale STFT | 6-W`$_2$` |  <span style="color:gray">`$\mathbf{-7.4\pm 9.9} $`</span> |  `$ \mathbf{-17.5\pm 19.8} $` |  `$ -24.4\pm 15.8  $` | <span style="color:gray">`$\mathbf{-2.27\pm6.85}$`</span> | <span style="color:gray">`$ 1.46\pm8.39 $`</span> | `$ 5.74\pm5.23 $` |
					| 1-W`$_2$` but MHU-Net  | 7-W`$_2$` | `$ -8.1\pm9.1 $` | `$ -23.7\pm20.4 $` | `$ -20.9\pm15.8 $` | `$ -3.29\pm6.35 $` |  `$ 0.07\pm8.22 $` |  `$ 5.75\pm4.53 $` |
					</textarea>
				</section>

				<section data-markdown  style="font-size: medium">
					<textarea data-template>
					## Label conditioning results

					|Method | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
					| :--- | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
					| w/o conditioning | 6-W`$_2$` | `$ -7.4\pm 9.9 $` | `$ -17.5\pm 19.8 $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ 1.46\pm8.39 $` | `$ 5.74\pm5.23 $` |
					| FiLM-bottleneck | 8-W`$_2$` | `$ -8.1\pm9.4 $` | `$ -19.1\pm18.1 $` | `$ -19.5\pm15.3 $` | `$ -2.77\pm6.64 $` | <span style="color:gray">`$ 1.17\pm8.24 $`</span> | <span style="color:gray">`$ 4.95\pm4.84 $`</span> |
					| FiLM-encoder | 9-W`$_2$` | `$ \mathbf{-3.5\pm6.1} $` | `$ -20.7\pm6.7 $` | `$ -4.4\pm6.8 $`  | `$ \mathbf{-1.68\pm5.47} $` | `$ -0.39\pm6.48 $` | `$ \mathbf{11.44\pm4.15} $` |
					| FiLM-final | 10-W`$_2$` | `$ -10.1\pm11.7 $` | `$ -34.5\pm28.4 $` | `$ \mathbf{-33.9\pm28.6 }$` | `$ -3.50\pm6.79 $` | `$ 0.29\pm9.14 $` | `$ 5.88\pm4.97 $` |
					| Label-multiply | 11-W`$_2$` | <span style="color:gray">`$ -7.2\pm9.8 $`</span> | <span style="color:gray">`$ \mathbf{-15.3\pm15.5 }$`</span> | `$ -18.6\pm20.3 $` | `$ -1.86\pm6.84 $` | `$ \mathbf{2.36\pm8.80} $` | `$ 5.13\pm4.07 $` |
					</textarea>
				</section>

				<section data-markdown  style="font-size: medium">
					<textarea data-template>
					## Visual conditioning results

					|Method | Frames | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
					| :--- | :---: | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
					|w/o conditioning | 0 | 6-W`$_2$` | `$ -7.4\pm 9.9 $` | `$ \mathbf{-17.5\pm 19.8} $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ \mathbf{1.46\pm8.39} $` | `$ 5.74\pm5.23 $` |
					|FiLM-encoder | 1 | 12-W`$_2$` | `$ -8.3\pm8.9 $` | `$ -26.1\pm17.5 $` | `$ -12.2\pm13.4 $` | `$ -3.38\pm5.54 $` | `$ -0.69\pm6.60 $` | `$ 5.91\pm4.25 $`  |
					|FiLM-bottleneck | 1 | 17-W`$_2$` | `$ -8.7\pm10.8 $` | `$ -20.7\pm20.1 $` | `$ \mathbf{-25.9\pm23.9} $` | `$ -3.02\pm7.10 $` | <span style="color:gray">`$ 1.17\pm8.95 $`</span> | <span style="color:gray">`$ 5.09\pm5.08 $`</span> |
					|FiLM-final | 1 | 18-W`$_2$` | `$ -4.8\pm7.4 $` | `$ -18.4\pm13.5 $` | `$ -10.9\pm8.7 $` | <span style="color:gray">`$ -2.24\pm6.22 $`</span> | `$ 0.05\pm7.91 $` | `$ 8.87\pm4.90 $` |
					|Final-multiply | 1 | 16-W`$_2$` | `${-4.1\pm7.2}$` | `$ -19.0\pm13.9 $` | `$ -15.4\pm14.3 $` | `$ \mathbf{-1.49\pm6.08} $` | `$ 0.98\pm8.39 $` | `$ 9.18\pm3.71 $` |
					|FiLM-final-lstm | 5 | 13-W`$_2$` | `$ \mathbf{-3.8\pm6.3} $` | `$ -22.7\pm12.6 $` | `$ -9.4\pm12.2 $` | `$ -2.51\pm5.32 $` | `$ -1.43\pm6.38 $` | `$ \mathbf{12.44\pm4.72} $` |
					|FiLM-bottleneck-lstm | 5 | 14-W`$_2$` | `$ -4.7\pm6.9 $` | `$ -17.9\pm10.7 $` | `$ -10.3\pm12.7 $` | `$ -1.55\pm5.94 $` | `$ 1.02\pm7.96 $` | `$ 7.90\pm3.27 $` |
					|FiLM-bottleneck-mp | 5 | 15-W`$_2$` | `$ -7.0\pm7.8 $` | `$ -23.0\pm14.4 $` | `$ -12.1\pm15.0 $` | `$ -2.46\pm5.82 $` | `$ 0.54\pm7.66 $` | `$ 6.28\pm3.61 $` |
					|SoP-unet7 | 3 | 19 | `$-18.7\pm8.9$` | `$-21.1\pm9.4$` | n/a | `$-3.76\pm4.00$` | `$-1.45\pm4.68$` | `$7.56\pm3.13$` |
					|SoP-unet7-ft | 3 | 20 | `$-17.5\pm8.5$` | `$-20.3\pm9.3$` | n/a | `$-2.57\pm4.99$` | `$0.47\pm6.43$` | `$6.89\pm2.48$` |
					|SoP-unet5-Solos | 3 | 21 | `$-16.9\pm8.6$` | `$-18.7\pm8.9$` | n/a | `$-2.9\pm4.7$` | `$-1.67\pm5.34$` | `$11.07\pm6.87$` |
					</textarea>
				</section>

			</section>

			<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
				<h2>IV - Conclusion</h2>
			</section>

			<section>
				<h3>Research problem I</h3>
				<ul>
					<li class="question">Where can we merge different data representations?</li>
					<li class="affirmation fragment"> Late fusion and hybrid fusion for high-level concepts</li>
					<li class="affirmation fragment"> Early fusion for continuous and strong conditioning</li>
				</ul>
			</section>

			<section>
					<h3>Research problem II</h3>
					<ul>
						<li class="question">How should we merge data from different sources?</li>
						<li class="affirmation fragment"> Strictly task-dependent</li>
					</ul>
			</section>

			<section>
				<h3>Contributions</h3>
				<div class="references">
					<h5 style="text-align: left; margin-bottom: 0em; margin-left: 3em">Audio-Visual MIR</h5>
					<ul style="font-size: smaller">
						<li style="font-weight:bold;">O. Slizovskaia, E. Gómez, and G. Haro. (SMC, 2016) Automatic musical instrument recognition in audiovisual recordings by combining image and audio classification strategies.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/SMC2016</a></span>
						</li>
						<li style="font-weight:bold;">O. Slizovskaia, E. Gómez, and G. Haro. (ACM ICMR, 2017) Musical instrument recognition in user-generated videos using a multimodal convolutional neural network architecture.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/ICMR2017</a></span>
						</li>
						<li style="font-weight:bold;">O. Slizovskaia, E. Gómez, and G. Haro. (ISMIR-LBD, 2017) Correspondence between audio and visual deep models for musical instrument detection in video recordings.</li>
						<li style="font-weight: bold;">O. Slizovskaia, E. Gómez, and G. Haro. (ICML-ML4M, 2018) A Case Study of Deep-Learned Activations via Hand-Crafted Audio Features.</li>
						<li style="font-weight: bold;">O. Slizovskaia, L. Kim, E. Gómez, and G. Haro. (ICASSP, 2019) End-to-end sound source separation conditioned on instrument labels.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/vimss</a></span>
						</li>
						<li>J.F. Montesinos, O. Slizovskaia, and G. Haro. (IEEE MMSP, 2020) Solos: A Dataset for Audio-Visual Music Analysis.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/JuanFMontesinos/Solos</a></span>
						</li>
						<li style="font-weight: bold;">O. Slizovskaia, G. Haro and E. Gómez. (IEEE/ACM TASLP, 2020) Conditioned Source Separation for Musical Instrument Performances (under review after major changes).
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/conditioned-u-net</a></span>
						</li>
					</ul>
					<h5 style="text-align: left; margin-top: 1em; margin-bottom: 0em; margin-left: 3em">Collaborations</h5>
					<ul style="font-size: smaller">
						<li>J. Pons, O. Slizovskaia, R. Gong, E Gómez, X. Serra. (EUSIPCO, 2017) Timbre analysis of music audio signals with convolutional neural networks.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/EUSIPCO2017</a></span>
						</li>
						<li>E. Fonseca, R. Gong, D. Bogdanov, O. Slizovskaia, E. Gómez, X. Serra. (DCASE, 2017) Acoustic scene classification by ensembling gradient boosting machine and convolutional neural networks. </li>
						<li>R. Gong, E. Fonseca, D. Bogdanov, O. Slizovskaia, E. Gómez, X. Serra. (DCASE, 2017) Acoustic scene classification by fusing LightGBM and VGG-net multichannel predictions. </li>
						<li>J. Serrà, D. Álvarez, V. Gómez, O. Slizovskaia, J.F. Núñez, and J. Luque. (ICLR, 2020) Input complexity and out-of-distribution detection with likelihood-based generative models. </li>
						<li>D. Michelsanti, O. Slizovskaia, G. Haro, E. Gómez, Z.H. Tan, and J. Jensen. (INTERSPEECH, 2020) Vocoder-Based Speech Synthesis from Silent Videos. </li>
					</ul>
				</div>
			</section>

			<section class="cover" data-background="img/mix1.png" data-state="no-title-footer no-progressbar has-dark-background">
			<aside class="notes"> See Integrated Gradients at https://captum.ai/docs/captum_insights</aside>
				Thank you!
			</section>

			<!--
			<section>
				<h3><span style="color: #ff637d">Thank you!</span></h3>
				<br>
				<span style="color: orange">Courtesy </span>
				<br>
				<ul>
					<li> URMP Dataset: <br><a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a></li>
					<li> TensorFlow Research Cloud and Jeju DL Camp</li>
					<li> Maria de Maeztu program</li>
					<li> GitHub: <a style="font-size: smaller;" href="https://github.com/Veleslavia/vimss">https://github.com/Veleslavia/vimss</a></li>
				</ul>
			</section>
			-->

			<div class="footer"></div>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies



		Reveal.initialize({

			math: {
				  mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				  config: 'TeX-AMS_HTML-full',
				  // pass other options into `MathJax.Hub.Config()`
				  TeX: { extensions: ["color.js"], Macros: { RR: "{\\bf R}" }
				  },
			},
			// specified using percentage units.
			//width: 960,
			//height: 700,
			//controls: false,
			progress: true,
			//history: true,
			//center: false,
			slideNumber: true,
			hash: true,
			//minScale: 0.1,
			//maxScale: 5,
			//transition: 'none', //
			audio: {
				prefix: 'audio/', 	// audio files are stored in the "audio" folder
				suffix: '.wav',		// audio files have the ".wav" ending
				textToSpeechURL: null,  // the URL to the text to speech converter
				defaultNotes: false, 	// use slide notes as default for the text to speech converter
				defaultText: false, 	// use slide text as default for the text to speech converter
				advance: -10, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance 
				autoplay: true,	// automatically start slideshow
				defaultDuration: 10,	// default duration in seconds if no audio is available 
				defaultAudios: false,	// try to play audios with names such as audio/1.2.ogg
			},
			plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealMath],
			dependencies: [
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true },
				{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function () { return !!document.body.classList; } },
				{ src: 'plugin/highlight/highlight.js', async: true }
			]
		});
		</script>
	</body>
</html>
