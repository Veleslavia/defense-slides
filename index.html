<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Olga Slizovskaia PhD thesis defense 21 October 2020</title>


		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="css/theme/upf.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<aside class="notes">
						The complementary relationship between audio and video data drives multimodal studies across
						various domains and problems. Within that scope, we emphasise the important role that visual
						modality plays in music perception and conduct a study on multimodal data fusion techniques in
						the music information retrieval domain. In this talk, we outline challenges in audio-visual
						music information retrieval and present our work that addresses instrument classification and
						source separation problems in musical instrument performances. We discuss the efficiency of
						conditioning techniques being applied at different levels of a primary network and make use of
						two extra modalities of data, namely instrument labels and the corresponding visual stream data.
					</aside>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<img src="img/image52.jpg" class="logo" style="height: 50px">
					<img src="img/image56.png" class="logo" style="height: 50px">
					<img src="img/image54.jpg" class="logo" style="height: 50px">

					<h2>Audio-Visual Deep Learning Methods for <br> Musical Instrument Classification
						and Separation</h2>
					<p><em>PhD candidate:</em> Olga Slizovskaia </p>
					<div class="multiCol" style="margin-left: 5em">
						<div class="col" style="font-size: smaller">
							<p><em>Supervisors:</em></p>
							Dr. Emilia Gómez <h4>Joint Research Centre, EC <br> Music Technology Group, UPF</h4>
							<p></p>
							Dr. Gloria Haro <h4>Image Processing Group, UPF</h4>
						</div>
						<div class="col" style="font-size: smaller">
							<p><em>Committee:</em></p>
							Dr. Xavier Giró-i-Nieto <h4>Universitat Politècnica de Catalunya</h4>
							<p></p>
							Dr. Xavier Serra <h4>Universitat Pompeu Fabra</h4>
							<p></p>
							Dr. Estefanía Cano <h4>Agency for Science, Research and Technology, <br> A*STAR, Singapore</h4>
						</div>
					</div>
					<p><em>21/10/2020</em></p>
					<aside class="notes">
						Dear commitee, my supervisors and everyone. I'm proud to be here with you today and to present
						my PhD thesis titled "Audio-Visual Deep Learning Methods for Musical Instrument Classification
						and Separation".

						At first, I will talk about what is audio-visual deep learning, what are the task which we are
						interested in, why you should care about multimodal methods and multimodal deep learning at all
						I'll explain why it is not so straightforward as it may look and present some of my work.
					</aside>
				</section>
				<section style="text-align: left">
					<h3>Outline</h3>
					<h2>I - State of the Art: Audio-Visual Machine Learning and MIR</h2>
					<h2>II - Audio-Visual Musical Instrument Classification </h2>
					<h2>III - Audio-Visual Source Separation </h2>
					<h2>IV - Conclusion</h2>
				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>I - Audio-Visual ML and MIR</h2>
				</section>

				<section>
					<aside class="notes">
						So, what is audio-visual learning and why should you care about it? (Don't trust me, maybe we
						should not care about it at all).
						First of all, we should care about AV methods because neither machine perception nor human
						perception is perfect and  while machine perception is trying to mimic human perception sometimes with success,
						many problems remain open, especially for complex scenes that involve several perception channels.
						Music one such phenomenon, it's multimodal by nature from real-life performance to modern music videos.
						And once we try to analize it, we focus on the following tasks:

					</aside>
						<h3>Why should we care about Audio-Visual MIR?</h3>
						<ul>
							<li class="fragment">Music is multimodal, we aim for better understanding and analysis</li>
							<li class="fragment">Humans are very good at merging different sources of information</li>
							<li class="fragment">Yet, ML algorithms are not as good as humans</li>
							<li class="fragment">Various practical applications (alignment, transcription, separation, localization, tagging etc.)</li>
						</ul>
				</section>

				<section>
					<aside class="notes">
					</aside>
						<h3>What are the challenges in audio-visual MIR?</h3>
						<ul>
							<li class="fragment">Shortage of dedicated datasets</li>
							<li class="fragment">Low quality of available data, large diversity in data</li>
							<li class="fragment">Dimensionality mismatch problem</li>
							<li class="fragment">Data aggregation problem</li>
						</ul>
				</section>

				<section>
					<aside class="notes">
					</aside>
					<section>
						<h3>Research question I</h3>
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<br>
						<h3> Early fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/earlyfusion.png">
					</section>
					<section data-transition="none">
						<br>
						<h3> Late fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/latefusion.png">
					</section>
					<section data-transition="none">
						<br>
						<h3> Hybrid fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/hybridfusion.png">
					</section>
				</section>

				<section>
						<h3>Research question II</h3>
						<ul>
							<li class="question" style="margin-bottom: 1em">How should we merge data from different sources?</li>
						</ul>
						<ul>
							<li class="fragment">
								Concatenation/additive conditioning</li>
							<li class="fragment">
								Multiplicative conditioning</li>
							<li class="fragment">
								Feature-wise Linear Modulation</li>
						</ul>
				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>II - Audio-Visual Musical Instrument Classification</h2>
				</section>

				<section>
					<aside class="notes">
						Video source: https://www.youtube.com/watch?v=aaxX9Ik6he0 CC0
					</aside>
					<h3> Problem definition </h3>
					<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/problem_definition_informal.png">
				</section>

				<section>
					<aside class="notes">
						For the classification setup, we have a set of objects $X \in \mathbb{R}^N$ and a set of class
						labels $C \in \{0,1\}^K$. We are interested in predicting a final subset of
						labels $y=(y_1, y_2, ..., y_K) \in C$ for a sample $x=(x_1, x_2, ..., x_N) \in X$ which is
						an $N$-dimensional representation of an object $x$. Therefore, we are looking for a function such as

						where $\theta$ are the function parameters and $ \hat{y} $ is a probability vector for the
						class estimates. A common mapping function $ f $ could be a complex non-linear neural
						network, where the final probability scores can be obtained via softmax function
						\[ \hat{y_j}(z) = \frac{e^{z_j}}{\sum_{i=1}^L e^{z_i}}, \] which takes a vector of
						arbitrary real-valued scores $z = (z_1, ... z_L) \in \mathbb{R}^L $ and transforms
						it to a vector of values between zero and one that sum to one.

						A common choice to learn the parameters $\theta$ is through a backpropagation
						algorithm where one of the common  optimization techniques can be used to
						minimize a loss function. For multi-labels classification,
						the loss function can be defined as categorical cross-entropy between ground truth and
						estimated probability distributions of class labels:
					</aside>
					<h3> Formal problem definition </h3>

					<ul>
						<li class="fragment"> set of objects $ \definecolor{signal}{RGB}{18,110,213} {\color{signal} X} \in \mathbb{R}^N$,
							samples $ {\definecolor{signal}{RGB}{18,110,213} \color{signal}x}=({\color{signal} x_1, x_2, ..., x_N}) \in {\color{signal}X}$ </li>
						<li class="fragment"> set of class labels $\definecolor{categories}{RGB}{203,23,206} {\color{categories}C} \in \{0,1\}^{\color{categories}K}$ </li>
						<li class="fragment"> final subset of labels $ \definecolor{classes}{RGB}{114,0,172}  \definecolor{categories}{RGB}{203,23,206}
							{\color{classes} y } =
							({\color{classes}y_1, y_2, ..., y_K}) \in {\color{categories} C }$ for a sample
							$ \definecolor{signal}{RGB}{18,110,213} \color{signal} x$ </li>
					</ul>
					<p class="fragment">
					\[
					\begin{aligned}
					\definecolor{classes}{RGB}{114,0,172}
					\definecolor{params}{RGB}{45,177,93}
					\definecolor{model}{RGB}{251,0,29}
					\definecolor{signal}{RGB}{18,110,213}
					\definecolor{probability}{RGB}{217,86,16}
					\definecolor{categories}{RGB}{203,23,206}

					{\color{probability} \hat{y}} = {\color{model} f}_{\color{params} \theta}({\color{signal} x})
					\end{aligned}
					\]

						<span style="color: rgb(45,177,93)"> find parameters</span>
						<span style="color: rgb(251,0,29)"> for a model </span> that gives <br>
						<span style="color: rgb(203,23,206)">class </span>
						<span style="color: rgb(217,86,16)">estimation probabilities </span>
						<span style="color: rgb(18,110,213)">for a sample</span>

					</p>

					<p class="fragment">
						\[

						\definecolor{loss}{RGB}{128,121,14}
						\definecolor{classes}{RGB}{114,0,172}
						\definecolor{probability}{RGB}{217,86,16}
						\definecolor{categories}{RGB}{203,23,206}

							{ \color{loss} \mathcal{L} } ( {\color{classes} y}, {\color{probability} \hat{y}})
						= -{\color{loss} \sum}_{\color{categories} i=1}^{\color{categories} K} {\color{classes}y_i} {\color{loss} * \log}( {\color{probability} \hat{y}_i })
						\]

						<span style="color: rgb(203,23,206)">for multi-labels classification, </span>
						minimize <span style="color: rgb(203,23,206)"> categorical </span>
						<span style="color: rgb(128,121,14)"> cross-entropy between </span>
						<span style="color: rgb(114,0,172)"> ground truth </span>
						<span style="color: rgb(128,121,14)"> and </span>
						<span style="color: rgb(217,86,16)"> estimated probability distributions </span>
						<span style="color: rgb(203,23,206)"> of class labels </span>
					</p>


				</section>

				<section>
					<h3> First multimodal approach </h3>
					<ul>
						<li class="fragment"> Hybrid/Late fusion</li>
						<li class="fragment"> Concatenation</li>
					</ul>
					<img class="fragment" height="300" style="margin-left: 2em; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/icmr_multimodal.png">

				</section>


				<section data-transition="none">
					<h3> First multimodal approach </h3>
					<img height="200" style="margin-left: 2em; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/icmr_multimodal.png">

					<div class="multiCol">
						<div class="col fragment">
						<ul>
							<li> MelSpec audio representation </li>
							<li> Architectures</li>
								<ul>
									<li> Han et al., 1.5M params</li>
									<li> Choi et al., 2.4M params</li>
									<li> Xception, 9.6M params</li>
								</ul>
						</ul>
						</div>

						<div class="col fragment">
						<ul>
							<li> Bag-of-frames RGB</li>
							<li> Number of frames: 10-100 </li>
								<ul>
									<li> FCVID: 20-100 </li>
									<li> YouTube-8M: 10-20 </li>
								</ul>
							<li>Fine-tune or train from scratch</li>
						</ul>
						</div>
					</div>

					<blockquote class="fragment">
						DATASETS
						<ul>
							<li> FCVID: Musical Performance With Instruments <br> &emsp; 12 classes, 5K videos, 260 hours </li>
							<li> YouTube-8M: MusInstr-Normalized <br> &emsp; 46 classes, 60k videos, 4k hours </li>
						</ul>
					</blockquote>

				</section>

				<section>
					<h3>Results</h3>
					<section data-transition="none" data-markdown>
						<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | **88.27** | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | 87.25 | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | 78.95 | `$+5.21$` | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | **84.69** | `$-0.43$` | `$-13.6$` |

							 > <!-- .element: class="fragment" --> comparing Xception and Choi et al. as they perform compatible as audio-only models

						</textarea>

					</section>

					<section data-transition="none" data-markdown>
						<aside class="notes">Choi is smaller and have stronger regularization, stronger features </aside>

						<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | **88.27** | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | 87.25 | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | 78.95 | <span style="background-color: #c8102e; border-radius: 5px"> &nbsp; `$+5.21^*$` </span> | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | **84.69** | `$-0.43$` | `$-13.6$` |

							<span style="background-color: #c8102e; border-radius: 5px"> &nbsp; `${}^*$` Issues with YouTube-8M annotations &nbsp; </span> <!-- .element: class="fragment" -->

							 > <!-- .element: class="fragment" --> TAKE AWAY:
							 > multi-modal fusion is beneficial overall <br>
							 > concatenation is not enough for controversial inputs

						</textarea>

					</section>

					<section data-markdown>
						<textarea data-template>
							### Audio-only results
							|Method | Params | Dataset | Hit@1 | Hit@3 | F1 |
							| :----- | -------- | --- | ---- | ----- | --- |
							| Han et al. | 1.5M |  FCVID  |  64.13 | 76.82 | 53.64 |
							| Choi et al. + CC | 2.4M |  FCVID |  77.73 | 92.05 | 77.18 |
							| Choi et al. + UC | 2.4M |  FCVID |  **79.81** | **96.09** | 78.71 |
							| Xception + UC | 9.6M |  FCVID | 78.69 | 94.44 | **79.35** |
							| Han et al. | 1.5M | YT-8M | 59.37 | 70.87 | 56.50 |
							| Choi et al. + UC | 2.4M | YT-8M | **83.58** | 94.23 | **84.26** |
							| Xception + UC | 9.6M | YT-8M | 83.53 | **94.69** | 84.16 |
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### Visual-only results

							|Dataset | FMs | PT | Steps | Time | Hit@1 | Hit@3 | F1 |
							| :----- | --- | --- | --- | ---- | ----- | ------ | --- |
							|FCVID  | 20 | No | 32K | 19h | 42.30 | 64.53 | 43.16 |
							|FCVID  | 30 | No | 16K | 11h | 65.39 | 81.75 | 67.29 |
							|FCVID  | 30 | Yes | 16K | 11h | 68.77 | 84.26 | 70.33 |
							|FCVID  | 50 | No | 24K | 22h | 67.47 | 83.21 | 69.38 |
							|FCVID  | 50 | Yes | 21K | 19h | **69.39** | **84.32** | **71.23** |
							|FCVID  | 100 | No | 43K | 98h | 68.56 | 83.97 | 70.42 |
							|FCVID  | 100 | Yes | 36K | 84h | 67.76 | 83.50 | 69.16 |
							|YT-8M  | 10 | No | 58K | 82h | 61.15 | 78.45 | 52.19 |
							|YT-8M  | 20 | Yes | 57K | 92h | **70.07** | **84.20** | **71.09** |
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Audio-visual results
							| Method | Dataset | Hit@1 | Hit@3 | F1 |
							| :----- | -------- | --- | ---- | ----- |
							|Xception @ 50 frames | FCVID | **88.28** | **97.00** | **88.27** |
							|Choi et al. @ 50 frames | FCVID | 86.97 | 96.09 | 87.25 | 
							|Xception @ 20 frames | YT-8M | 82.64 | 91.37 | 78.95 |
							|Choi et al. @ 20 frames | YT-8M | **84.01** | **93.41** | **84.69** |
						</textarea>
					</section>

				</section>


				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>III - Audio-Visual Source Separation</h2>
				</section>

				<section>
					<h2>Can you hear the difference?</h2>
					<br>
					<p>&nbsp;</p>
					<p>&nbsp;</p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_1.wav" > <span class="music">Sample 1</span> </p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_2.wav" > <span class="music">Sample 2</span></p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_3.wav" > <span class="music">Sample 3</span></p>
					<p>&nbsp;</p>
					<p>&nbsp;</p>
					<div class="remark">
						* All examples are taken from the URMP dataset: <a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a></li>
					</div>

					<aside class="notes">
						To introduce the topic I'm going to let you hear three short excerpts of the same composition
						(the same musical piece) and ask you a simple question.
						I believe that most you can hear a significant difference between sample 3 and the first two
						examples but... what it's much harder in case of sample 1 and sample 2.
						how many could hear the difference? The overlap is so strong that it's almoust impossible without
						special training.
						Now, I would like you to hear the actual difference between the three of them.
					</aside>
				</section>

				<section>
					<p>Can you hear the difference <span style="color: orange">now?</span></p>
					<br>

					<aside class="notes"> 
						This is how the first mixture is different from the second one.
						This is how the second mixture is different from the first one.
						This is residue of the third mixture with respect to the first two.

						Can you say if the first samples are from the same musical instrument or not?
						Would it be easier for you to distinguish between two recordings if I could provide you more information.... like this?
					</aside>

					<table>
						<thead>
							<tr>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_1_db.wav" >
									<span class="music">Separated 1</span>
								</span></th>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_2_vc.wav" >
									<span class="music">Separated 2</span>
								</span></th>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_3_cl.wav" >
									<span class="music"> Separated 3 </span>
								</span></th>
							</tr>
						</thead>
						<tbody>
							<tr class ="fragment">
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_1_db.gif"></td>
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_2_vc.gif"></td>
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_3_cl.gif"></td>
							</tr>
						</tbody>
					</table>

				</section>

				<section>
					<aside class="notes"> 
						Just keep it in mind for a little, while I'm continue to introduce the task which we're trying to solve.
						Having a single-channel recording of a musical performance, that is to say a mixture of many possibly overlapping signals, we want to estimate their individual tracks, their sources.
						It can be used for hearing aid, music production, music education, as a preprocessing for other analysis tasks and so on.
						Here is an illustration of our goal.  
					</aside>

						<h3> Problem definition </h3>
						<video controls>
							<source src="video/vimss_idea_demo.mp4" type="video/mp4">
						</video>
				</section>

				<section>
					<aside class="notes">
					Single channel source separation (SCSS) consists of estimating the individual sources
						$x_i$ given a mono mixture time-domain signal $y$ of $N$ sources:

					\begin{equation}
					y(t) = \sum_{i=1}^{N} x_i(t).
					\end{equation}

					Instead of predicting time-domain signals, a general approach for solving SCSS involves the
						estimation of $N$ masks for Short-Term Fourier transform (STFT) values of the mixture.
						In this case, we consider a time-frequency representation of the mixture $ \boldsymbol{Y} $
						and the  sources $ \boldsymbol{X_i} $, and the goal of the source separation method is to
						learn a real-valued (or complex-valued) mask $M_i$ for each source $i$ .

					Let us denote by $|\boldsymbol{X_i}(\tau, \omega)|$ and $|\boldsymbol{Y}(\tau, \omega)|$
						the magnitude of the STFT value, of $\boldsymbol{X_i}$ and $\boldsymbol{Y}$ respectively,
						at frequency $\omega$ and time frame $\tau$.
					In this work, we only consider two types of \textit{real-valued} masks,
						namely \textit{ideal ratio or soft} masks $M_i^{ir}$:
					\begin{equation}
						M_i^{ir}(\tau, \omega) = \frac{|\boldsymbol{X_i}(\tau, \omega)|}{|\boldsymbol{Y}(\tau, \omega)|},
					\end{equation}
					and \textit{ideal binary} masks
					$M_i^{ib}$:
					\begin{equation}
					\begin{gathered}
					M_i^{ib}(\tau, \omega) =
						\begin{cases}
						  1, & \text{if}\  \frac{|\boldsymbol{X_i}(\tau, \omega)|}{|\boldsymbol{Y}(\tau, \omega)| - |\boldsymbol{X_i}(\tau, \omega)|} \geq 1 \\
						  0, & \text{otherwise},
						\end{cases}
					\end{gathered}
					\end{equation}
					</aside>
					<h3> Formal problem definition </h3>

					<ul>
						<li class="fragment"> a mono mixture time-domain signal $y(t)$ </li>
						<li class="fragment"> individual sources $x_i(t)$ </li>
					</ul>
					<p class="fragment">
					\[
					\definecolor{classes}{RGB}{114,0,172}
					\definecolor{params}{RGB}{45,177,93}
					\definecolor{model}{RGB}{251,0,29}
					\definecolor{signal}{RGB}{18,110,213}
					\definecolor{probability}{RGB}{217,86,16}
					\definecolor{categories}{RGB}{203,23,206}

					y(t) = \sum_{i=1}^{N} x_i(t).

					\]

						the mixture equals to the sum of all sources

						estimate individual sources from the muxture
						<span style="color: rgb(45,177,93)"> find parameters</span>
						<span style="color: rgb(251,0,29)"> for a model </span> that gives <br>
						<span style="color: rgb(203,23,206)">class </span>
						<span style="color: rgb(217,86,16)">estimation probabilities </span>
						<span style="color: rgb(18,110,213)">for a sample</span>

					</p>

					<div class="multiCol">
						<div class="col fragment">
							direct waveform estimation
							<ul>
								<li >  </li>
								<li >  </li>
							</ul>
						</div>
						<div class="col fragment">
							masking-based approach
							<ul>
								<li >  </li>
								<li >  </li>
							</ul>
						</div>
					</div>
				</section>



				<section>
					<aside class="notes"> 
						Now I would like to shortly discuss the common approach to solve this problem and why it is difficult to solve.
						One of the problems is that sources often overlap and time and frequency. It's especially true for instruments playing in harmony and in unison, this is often the case for classical music. And this is also a problem for instruments of the same family, since they have very similar timbre as well. 
						
						It has been shown that the complexity of the task increases with the number of sources, and in our work we tackle the problem not of just two-channel source separation, but we address multiple instrument separation where actual number of sources is not known.

						For many years standard common approach for this task had consisted in estimating binary or ratio masks which could be applied to a magnitude of mixture spectrogram in order to obtain the magnitude spectrogram of the sources, and then recostruct the original sound using either the mixture phase of phase-reconstruction algorithm.
						Thus we lose some information which can be potentially usefull for source separation and this is our primary motivation for waveform domain source separation.

						On the other hand, with all this complexity and the first example I've shown you in mind, we want to answer another question:

						So we took week information, which is instrument labels information which can be available from another classifier network from taken from a different modality, a decided to use them to condition our model.  

					</aside>

					<section>
						<h2>Known challenges</h2>
						<ul>
							<li class="fragment">Same-family instruments are similar to one another</li>
							<li class="fragment">Unknown in advance number of sources</li>
							<li class="fragment">Complexity increases with the number of sources</li>
							<li class="fragment">Overlap in time and frequency between sources</li>
							<li class="fragment">Extra: inter-family mimicry for some instruments (clarinet vs. viola)</li>
						</ul>
						<br>
						<br>
						<p class="fragment" style="width: 80%; margin: auto; background-color: #606623; padding: 20px 0px 20px 10px; color:white">
							<span style="font-size:48px; float: left">💪</span>
							<span style="font-size:48px">Can we use extra information to improve separation? </span>
						</p>
					</section>
				</section>

				<section data-transition="none">
					<aside class="notes"> 
						Let me briefly explain the architecture.
						Overall, in deep learning based source separation, there is a huge variety of architecture design. 
						All that start from an encoder-decoder architecture (but STFT-based), when people started to use U-net with skip connections but also STFT-based,
						then Wave-U-Net appeared, and we inhereted Wave-U-Net model from their authors. 
						But, first of all, we are trying to estimated multiple sources which are very sparce, and, finally,
						we apply a feature modulation with instrument labels on the bottleneck of the U-Net.

						It's multiplicative conditioning, because back then the authors weren't aware of FiLM. We only applied it to the bottleneck although later we tried other combinations.   
					</aside>

					<section data-transition="none">
						<h3>Preliminary study</h3>
						<ul>
							<li class="fragment">Multi-instrument Source Separation in Time Domain</li>
							<li class="fragment">Hybrid fusion</li>
							<li class="fragment">Multiplicative Conditioning on Instrument Labels</li>
						</ul>
					</section>


					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
							<img class="fragment" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net1.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net2.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net3.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net4.png">
					</section>
				</section>

				<section> 
					<aside class="notes"> 
						This was trained with a simple MSE loss between original and reconstructed sources, it was trained on a multi-modal dataset of music performances from Rochester university, which is relatively small and has its issues. 
						So, just a disclaimer, the results which I'm going to show you now are results of the network which has been trained with 30 videos which is about 1,5h in total.
					</aside>

					<section>
						<h3> Experiments </h3>
						<h4 class="fragment">URMP Dataset <a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a></h4>
						<ul class="fragment">
							<li>13 instruments</li>
							<li>44 videos -> 40 valid pieces </li>
							<li>12 duest, 20 trios, 8 quartets </li>
							<li>87 unique audio tracks </li>
							<li>Train/Test split: 30/10 pieces </li>
						</ul>

					</section>

					<section>
						<h3>Video Demo: Trained on 30 videos</h3>
						<video controls>
							<source src="video/urmp_demo_multisource.mp4" type="video/mp4">
						</video>
					</section>

					<section>
						<aside class="notes"> 
							We compared our method with InformedNMF approch, which was the closest method able to separate sounds of different instruments and it uses pretrained timbre models. And, surprisingly, our method performs better in SIR.
						</aside>

						<h3>Summary of results</h3>
						<table>
							<thead>
								<tr>
									<th>Method</th>
									<th>SDR</th>
									<th>SIR</th>
									<th>SAR</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td><span style="color: #57ba98">InformedNMF</span></td>
									<th><span style="color: #aaabb8"><em>-0.16</em></span></th>
									<th>1.42</th>
									<th>9.31</th>
								</tr>
								<tr>
									<td><span style="color: orange">Exp-Wave-U-Net</span></td>
									<th>-4.12</th>
									<th>-3.06</th>
									<th><span style="color: #aaabb8"><em>12.18</em></span></th>
								</tr>
								<tr>
									<td><span style="color: #ff637d">CExp-Wave-U-Net</span></td>
									<th>-1.37</th>
									<th><span style="color: #aaabb8"><em>2.16</em></span></th>
									<th>6.36</th>
								</tr>
							</tbody>
						</table>
					</section>

				<section>

					<aside class="notes"> 
						But we achieve the most notable difference with the baseline when the complexity of the problem increases. 
						Here you can see that InformedNMF outperforms our model when we test task is the separation of two instruments, but as the number of sources increases, the conditioned wave-u-net notably outperforms the baseline.
					</aside>

					<h3>Summary of results</h3>
					<img data-src="img/sdr.png" height="300px" style="border:1px solid #555a5f">
					<img data-src="img/sir.png" height="300px" style="border:1px solid #555a5f">
					<!--<img data-src="img/sar.png" height="250px" style="border:1px solid #555a5f">-->
				</section>

				<section>

					<aside class="notes">
						Take away note: it was inspiring but we lacked TPUs and scaling was not an option
					</aside>

					<h3>Summary of results</h3>
					<blockquote>Take away note: it was inspiring but we lacked TPUs and scaling was not an option
					</blockquote>
				</section>

				<section>

					<aside class="notes"> 
						As I said, we took the original Wave-U-Net model and did quite a couple of changes. 
						For example, we optimized the learning rate, we optimized data loading pipeline, we ported it such that it was possible to train on GC TPUs (which we were able to use at the time), also tested half-precision, which resulted in 35 times faster training comparing to the original model. 
					</aside>

					<h3> Ablation and Speedup</h3>
					<p class="fragment">Learning rate</p>
					<p class="fragment">TPUs</p>
					<p class="fragment">tf.float32 vs tf.float16</p>
					<p class="fragment" style="font-size: larger; color: orange">Total Speedup: x35.4</p>
				</section>
			</section>

			<section>
				<p>What we did next...</p>

				<!--
				<p class="fragment">
					<a style="font-size: smaller;" href="https://veleslavia.github.io/conditioned-u-net/">https://veleslavia.github.io/conditioned-u-net/</a>
				</p>
				-->
			</section>

			<section>
				<h3> SOLOS </h3>
				<ul>
					<li class="fragment" data-fragment-index="1">13 instruments matching the URMP dataset</li>
					<li class="fragment" >Only solo performances </li>
					<li class="fragment" >Semi-automatic and manual quality control </li>
					<li class="fragment" >Mix-and-separate approach</li>
					<li class="fragment" >755 individual recordings </li>
					<li class="fragment" >Extra: valid timestamps and skeletons  </li>
				</ul>

				<p>&nbsp;</p>
				<p>&nbsp;</p>
				<div class="remark fragment" data-fragment-index="1">
					* Joint work with Juan Montesinos <a style="font-size: smaller;" href="https://juanmontesinos.com/Solos">https://juanmontesinos.com/Solos</a></li>
				</div>

			</section>

			<section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img class="fragment" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net1.png">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net2.png">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net3.png">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net4.png">
				</section>
			</section>

			<section>
				<section>
					<h3>Results</h3>
				</section>
				<section data-markdown  style="font-size: x-small">
					<textarea data-template>
						|Method | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
						| :--- | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
						| IRM | U | `$ 13.1\pm5.4 $` | `$ 12.7\pm6.4 $`  | n/a | `$ 11.79\pm4.26 $` | `$ 19.94\pm5.59 $` | `$ 13.02\pm4.25 $` |
						| input mix | L | `$-3.7\pm5.7$` | `$-3.7\pm5.7$` | `$18.2\pm4.2$` | `$ -3.48\pm4.82 $` | `$ -3.20\pm4.95 $` | `$ 18.10\pm11.21 $` |
						| No filtering | 1 | `$  -2.1\pm 6.2 $` | `$ -15.2\pm 10.0 $` | `$ -8.1\pm 5.9 $` | `$ -0.77\pm5.76 $` | `$ 0.57\pm6.85 $` | `$ 10.88\pm3.21 $` |
						| Wiener (1 it.) | 1-W`$_1$` | `$ -3.9\pm 7.5 $` | `$ -16.0\pm 16.1 $` | `$ -15.1\pm 10.1 $` | `$ -1.01\pm6.20 $` | `$ 1.53\pm7.65 $` | `$ 7.70\pm3.81 $` |
						| Wiener (2 it.) | 1-W`$_2$` | `$ -7.6\pm 10.7 $` | `$ -21.4\pm 24.1 $` | `$ -25.4\pm 16.8 $` | `$ -2.56\pm7.10 $` | `$ \mathbf{1.72\pm8.52} $` | `$ 4.99\pm5.62 $` |
						| 1-W`$_2$` with CL | 2-W`$_2$` | `$ -8.2\pm 10.9 $` |  `$ -24.1\pm 25.1  $` |  `$ -26.2\pm 17.8  $` |  `$ -3.06\pm7.06  $` | <span style="color:gray">`$ 1.38\pm8.55 $`</span> | <span style="color:gray">`$ 4.59\pm5.72 $`</span>  |
						| 2-W`$_2$` but binary masks | 3-W`$_2$` |  <span style="color:gray">`$ -9.7\pm 15.2 $`</span> | <span style="color:gray">`$ -21.6\pm 27.4 $`</span> |  `$ \mathbf{-36.2\pm 18.4}  $` |  `$ -3.82\pm7.95  $` |  `$ 0.09\pm7.10  $` |  `$ 4.93\pm7.53  $`  |
						| 1-W`$_2$` with noise | 4-W`$_2$` |  <span style="color:gray">`$ -7.6\pm 9.2 $`</span> |  `$-21.7\pm 20.4  $` |  `$ -21.7\pm 15.6 $` |  `$ -3.12\pm6.91  $` |  `$ 0.21\pm8.34  $` |  `$ \mathbf{6.04\pm5.32} $` |
						| 1-W`$_2$` no dB normalise | 5-W`$_2$` | `$ -10.1\pm 11.6 $` |  `$ -25.3\pm 24.9  $` |  `$ -27.0\pm 17.0  $` |  `$ -3.88\pm7.63  $` |  `$ 0.70\pm9.45  $` |  `$ 4.63\pm6.13  $` |
						| 1-W`$_2$` linear-scale STFT | 6-W`$_2$` |  <span style="color:gray">`$\mathbf{-7.4\pm 9.9} $`</span> |  `$ \mathbf{-17.5\pm 19.8} $` |  `$ -24.4\pm 15.8  $` | <span style="color:gray">`$\mathbf{-2.27\pm6.85}$`</span> | <span style="color:gray">`$ 1.46\pm8.39 $`</span> | `$ 5.74\pm5.23 $` |
						| 1-W`$_2$` but MHU-Net  | 7-W`$_2$` | `$ -8.1\pm9.1 $` | `$ -23.7\pm20.4 $` | `$ -20.9\pm15.8 $` | `$ -3.29\pm6.35 $` |  `$ 0.07\pm8.22 $` |  `$ 5.75\pm4.53 $` |
					</textarea>
				</section>

				<section data-markdown  style="font-size: x-small">
					<textarea data-template>
						|Method | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
						| :--- | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
						| w/o conditioning | 6-W`$_2$` | `$ -7.4\pm 9.9 $` | `$ -17.5\pm 19.8 $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ 1.46\pm8.39 $` | `$ 5.74\pm5.23 $` |
						| FiLM-bottleneck | 8-W`$_2$` | `$ -8.1\pm9.4 $` | `$ -19.1\pm18.1 $` | `$ -19.5\pm15.3 $` | `$ -2.77\pm6.64 $` | <span style="color:gray">`$ 1.17\pm8.24 $`</span> | <span style="color:gray">`$ 4.95\pm4.84 $`</span> |
						| FiLM-encoder | 9-W`$_2$` | `$ \mathbf{-3.5\pm6.1} $` | `$ -20.7\pm6.7 $` | `$ -4.4\pm6.8 $`  | `$ \mathbf{-1.68\pm5.47} $` | `$ -0.39\pm6.48 $` | `$ \mathbf{11.44\pm4.15} $` |
						| FiLM-final | 10-W`$_2$` | `$ -10.1\pm11.7 $` | `$ -34.5\pm28.4 $` | `$ \mathbf{-33.9\pm28.6 }$` | `$ -3.50\pm6.79 $` | `$ 0.29\pm9.14 $` | `$ 5.88\pm4.97 $` |
						| Label-multiply | 11-W`$_2$` | <span style="color:gray">`$ -7.2\pm9.8 $`</span> | <span style="color:gray">`$ \mathbf{-15.3\pm15.5 }$`</span> | `$ -18.6\pm20.3 $` | `$ -1.86\pm6.84 $` | `$ \mathbf{2.36\pm8.80} $` | `$ 5.13\pm4.07 $` |
					</textarea>
				</section>

				<section data-markdown  style="font-size: x-small">
					<textarea data-template>
						|Method | Frames | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
						| :--- | :---: | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
						|w/o conditioning | 0 | 6-W`$_2$` | `$ -7.4\pm 9.9 $` | `$ \mathbf{-17.5\pm 19.8} $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ \mathbf{1.46\pm8.39} $` | `$ 5.74\pm5.23 $` |
						|FiLM-encoder | 1 | 12-W`$_2$` | `$ -8.3\pm8.9 $` | `$ -26.1\pm17.5 $` | `$ -12.2\pm13.4 $` | `$ -3.38\pm5.54 $` | `$ -0.69\pm6.60 $` | `$ 5.91\pm4.25 $`  |
						|FiLM-bottleneck | 1 | 17-W`$_2$` | `$ -8.7\pm10.8 $` | `$ -20.7\pm20.1 $` | `$ \mathbf{-25.9\pm23.9} $` | `$ -3.02\pm7.10 $` | <span style="color:gray">`$ 1.17\pm8.95 $`</span> | <span style="color:gray">`$ 5.09\pm5.08 $`</span> |
						|FiLM-final | 1 | 18-W`$_2$` | `$ -4.8\pm7.4 $` | `$ -18.4\pm13.5 $` | `$ -10.9\pm8.7 $` | <span style="color:gray">`$ -2.24\pm6.22 $`</span> | `$ 0.05\pm7.91 $` | `$ 8.87\pm4.90 $` |
						|Final-multiply | 1 | 16-W`$_2$` | `${-4.1\pm7.2}$` | `$ -19.0\pm13.9 $` | `$ -15.4\pm14.3 $` | `$ \mathbf{-1.49\pm6.08} $` | `$ 0.98\pm8.39 $` | `$ 9.18\pm3.71 $` |
						|FiLM-final-lstm | 5 | 13-W`$_2$` | `$ \mathbf{-3.8\pm6.3} $` | `$ -22.7\pm12.6 $` | `$ -9.4\pm12.2 $` | `$ -2.51\pm5.32 $` | `$ -1.43\pm6.38 $` | `$ \mathbf{12.44\pm4.72} $` |
						|FiLM-bottleneck-lstm | 5 | 14-W`$_2$` | `$ -4.7\pm6.9 $` | `$ -17.9\pm10.7 $` | `$ -10.3\pm12.7 $` | `$ -1.55\pm5.94 $` | `$ 1.02\pm7.96 $` | `$ 7.90\pm3.27 $` |
						|FiLM-bottleneck-mp | 5 | 15-W`$_2$` | `$ -7.0\pm7.8 $` | `$ -23.0\pm14.4 $` | `$ -12.1\pm15.0 $` | `$ -2.46\pm5.82 $` | `$ 0.54\pm7.66 $` | `$ 6.28\pm3.61 $` |
						|SoP-unet7 | 3 | 19 | `$-18.7\pm8.9$` | `$-21.1\pm9.4$` | n/a | `$-3.76\pm4.00$` | `$-1.45\pm4.68$` | `$7.56\pm3.13$` |
						|SoP-unet7-ft | 3 | 20 | `$-17.5\pm8.5$` | `$-20.3\pm9.3$` | n/a | `$-2.57\pm4.99$` | `$0.47\pm6.43$` | `$6.89\pm2.48$` |
						|SoP-unet5-Solos | 3 | 21 | `$-16.9\pm8.6$` | `$-18.7\pm8.9$` | n/a | `$-2.9\pm4.7$` | `$-1.67\pm5.34$` | `$11.07\pm6.87$` |
					</textarea>
				</section>

			</section>

			<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
				<h2>IV - Conclusion</h2>
			</section>

			<section>
				<h3>Research question I</h3>
				<ul>
					<li class="question">Where can we merge different data representations?</li>
				</ul>
			</section>

			<section>
					<h3>Research question II</h3>
					<ul>
						<li class="question">How should we merge data from different sources?</li>
					</ul>
			</section>

			<section>
				<h3>Contributions</h3>
				<div class="references">
				<ul>
					<li style="font-weight:bold;">O. Slizovskaia, E. Gómez, and G. Haro. (SMC, 2016) Automatic musical instrument recognition in audiovisual recordings by combining image and audio classification strategies.</li>
					<li style="font-weight:bold;">O. Slizovskaia, E. Gómez, and G. Haro. (ACM ICMR, 2017) Musical instrument recognition in user-generated videos using a multimodal convolutional neural network architecture.</li>
					<li style="font-weight:bold;">O. Slizovskaia, E. Gómez, and G. Haro. (ISMIR-LBD, 2017) Correspondence between audio and visual deep models for musical instrument detection in video recordings.</li>
					<li style="font-weight: bold;">O. Slizovskaia, E. Gómez, and G. Haro. (ICML-ML4M, 2018) A Case Study of Deep-Learned Activations via Hand-Crafted Audio Features.</li>
					<li style="font-weight: bold;">O. Slizovskaia, L. Kim, E. Gómez, and G. Haro. (ICASSP, 2019) End-to-end sound source separation conditioned on instrument labels.</li>
					<li>J.F. Montesinos, O. Slizovskaia, and G. Haro. (IEEE MMSP, 2020) Solos: A Dataset for Audio-Visual Music Analysis.</li>
					<li>D. Michelsanti, O. Slizovskaia, G. Haro, E. Gómez, Z.H. Tan, and J.Jensen. (INTERSPEECH, 2020) Vocoder-Based Speech Synthesis from Silent Videos. </li>
					<li style="font-weight: bold;">O. Slizovskaia, G. Haro and E. Gómez. (IEEE/ACM TASLP, 2020) Conditioned Source Separation for Musical Instrument Performances (Under review).</li>
				</ul>
				</div>
			</section>

			<section class="cover" data-background="img/mix1.png" data-state="no-title-footer no-progressbar has-dark-background">
			<aside class="notes"> See Integrated Gradients at https://captum.ai/docs/captum_insights</aside>
				Thank you!
			</section>

			<!--
			<section>
				<h3><span style="color: #ff637d">Thank you!</span></h3>
				<br>
				<span style="color: orange">Courtesy </span>
				<br>
				<ul>
					<li> URMP Dataset: <br><a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a></li>
					<li> TensorFlow Research Cloud and Jeju DL Camp</li>
					<li> Maria de Maeztu program</li>
					<li> GitHub: <a style="font-size: smaller;" href="https://github.com/Veleslavia/vimss">https://github.com/Veleslavia/vimss</a></li>
				</ul>
			</section>
			-->

			<div class="footer"></div>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies



		Reveal.initialize({

			math: {
				  mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				  config: 'TeX-AMS_HTML-full',
				  // pass other options into `MathJax.Hub.Config()`
				  TeX: { extensions: ["color.js"], Macros: { RR: "{\\bf R}" }
				  },
			},
			// specified using percentage units.
			//width: 960,
			//height: 700,
			//controls: false,
			progress: true,
			//history: true,
			//center: false,
			slideNumber: true,
			hash: true,
			//minScale: 0.1,
			//maxScale: 5,
			//transition: 'none', //
			audio: {
				prefix: 'audio/', 	// audio files are stored in the "audio" folder
				suffix: '.wav',		// audio files have the ".wav" ending
				textToSpeechURL: null,  // the URL to the text to speech converter
				defaultNotes: false, 	// use slide notes as default for the text to speech converter
				defaultText: false, 	// use slide text as default for the text to speech converter
				advance: -10, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance 
				autoplay: true,	// automatically start slideshow
				defaultDuration: 10,	// default duration in seconds if no audio is available 
				defaultAudios: false,	// try to play audios with names such as audio/1.2.ogg
			},
			plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealMath],
			dependencies: [
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true },
				{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function () { return !!document.body.classList; } },
				{ src: 'plugin/highlight/highlight.js', async: true }
			]
		});
		</script>
	</body>
</html>
