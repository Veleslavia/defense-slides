<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Olga Slizovskaia PhD thesis defense 21 October 2020</title>


		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="css/theme/upf.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<aside class="notes">
						The complementary relationship between audio and video data drives multimodal studies across
						various domains and problems. Within that scope, we emphasise the important role that visual
						modality plays in music perception and conduct a study on multimodal data fusion techniques in
						the music information retrieval domain. In this talk, we outline challenges in audio-visual
						music information retrieval and present our work that addresses instrument classification and
						source separation problems in musical instrument performances. We discuss the efficiency of
						conditioning techniques being applied at different levels of a primary network and make use of
						two extra modalities of data, namely instrument labels and the corresponding visual stream data.
					</aside>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<img src="img/image52.jpg" class="logo" style="height: 50px">
					<img src="img/image56.png" class="logo" style="height: 50px">
					<img src="img/image54.jpg" class="logo" style="height: 50px">

					<h2>Audio-Visual Deep Learning Methods for <br> Musical Instrument Classification
						and Separation</h2>
					<p><em>PhD candidate:</em> Olga Slizovskaia </p>
					<div class="multiCol" style="margin-left: 5em">
						<div class="col" style="font-size: smaller">
							<p><em>Supervisors:</em></p>
							Dr. Emilia GÃ³mez <h4>Joint Research Centre, EC <br> Music Technology Group, UPF</h4>
							<p></p>
							Dr. Gloria Haro <h4>Image Processing Group, UPF</h4>
						</div>
						<div class="col" style="font-size: smaller">
							<p><em>Committee:</em></p>
							Dr. Xavier GirÃ³-i-Nieto <h4>Universitat PolitÃ¨cnica de Catalunya</h4>
							<p></p>
							Dr. Xavier Serra <h4>Universitat Pompeu Fabra</h4>
							<p></p>
							Dr. EstefanÃ­a Cano <h4>Agency for Science, Research and Technology, <br> A*STAR, Singapore</h4>
						</div>
					</div>
					<p><em>21/10/2020</em></p>
					<aside class="notes">
						Dear commitee, my supervisors and everyone. I'm proud to be here with you today and to present
						my PhD thesis titled "Audio-Visual Deep Learning Methods for Musical Instrument Classification
						and Separation".

						At first, I will talk about what is audio-visual deep learning, what are the task which we are
						interested in, why you should care about multimodal methods and multimodal deep learning at all
						I'll explain why it is not so straightforward as it may look and present some of my work.
					</aside>
				</section>
				<section style="text-align: left">
					<h3>Outline</h3>
					<h2>I - Audio-Visual MIR</h2>
					<h2>II - Audio-Visual Musical Instrument Classification </h2>
					<h2>III - Audio-Visual Source Separation </h2>
					<h2>IV - Conclusion</h2>
				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>I - Audio-Visual MIR</h2>
				</section>

				<section data-state="is-in-background">
					<aside class="notes">
						First of all, audio-visual MIR.
						So, what is audio-visual learning and MIR and why do we study it?

						When I was 11 y.o.,
						I somehow passed through a regional selection stage and
						my music teacher brought me to compete to a prestigious-like
						 musical competition in my area.

						I did not proceed further but I remember the experience of whatching
						(watching, not listening) performances of other participant,
						especially the senior ones.

						It was a feast of performance techniques, both from a technical and artistic
						point of view, and I recall that we discussed for a couple of weeks everything
						that we saw and tried to adapt some tricks.

						That was my first immersive experience of how highly multimodal could be our perception
						of a music performance.

						But definitely not the last one. Many years have passed, but at any life concert,
						I watch maybe even more than I listen, especially if the scene is complex.
						You can narrow or widen the vision area and focus on a particular aspect of the performance.

						We (as a kind) are very efficient in aggregating information that comes from
						different perception channels in order to make sense of the word around us,
						and it is still unclear how we are doing it.

						And we should care about AV methods because while machine perception is trying to mimic human perception
						often with success, many problems remain open, especially for complex scenes that involve
						multi-sensory merging.

						Within the area of MIR, there are many tasks that humans do using the full range of perception
						capabilities and there are a number of tasks that can benifit from multi-modal approaches.

					</aside>
						<h3>Why do we study Audio-Visual MIR?</h3>
						<ul>
							<li class="fragment">Music is multimodal, we aim for better understanding and analysis</li>
							<li class="fragment">Humans are very good at merging different sources of information</li>
							<li class="fragment">Yet, ML algorithms are not as good as humans at multi-sensory merging</li>
							<li class="fragment">Various practical applications (alignment, transcription, separation, localization, tagging etc.)</li>
						</ul>
				</section>

				<section data-state="is-in-background">
					<aside class="notes">
						Now, once we are sure that it is important and have to be adressed,
						let us see, why it is not solved yet.

						Actually, first of all, because of Pareto principal.
						For all above tasks, audio-only based algorithms provide a good quality baseline.
						And in order to move from an audio-only approach to a multi-modal approach,
						there are some serious obstacles that one should overcome in order to
						benefit from a multi-modal approach (and maybe the quality/performance boost
						that will be obtained is not worth the effort).
						In AV MIR (niche area) the challenges are the following.
					</aside>
						<h3>What are the challenges in Audio-Visual MIR?</h3>
						<ul>
							<li class="fragment">Shortage of dedicated datasets</li>
							<li class="fragment">Low quality of available data, large diversity in data</li>
							<li class="fragment">Dimensionality mismatch problem</li>
							<li class="fragment">Data aggregation problem</li>
							<li class="fragment">Extra: limited computing power</li>
						</ul>
				</section>

				<section data-state="is-in-background">
					<aside class="notes"> The primary research objective of this study is
					to propose modern and efficient audio-visual methods for MIR problems.
					</aside>
					<h3> Research goals </h3>
				</section>

				<section data-state="is-in-background">
					<aside class="notes">
						Fro the last three points in challenges,

						the answer would be strictly related to data representation
						and  learned representation ( is unseparable from the idea )

						So another question that we should ask is:.

					</aside>
					<section data-transition="none">
						<h3>Research focus (I/II)</h3>
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<aside class="notes">There are three common and well known
						approaches in the literature, which are used.

						For early fusion we mix data representation from different perception channels as soon
							as possible and process and optimize the mixed representation
							in order to solve the underlying task.
						</aside>
						<br>
						<h3> Early fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/earlyfusion.png">
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<br>
						<h3> Late fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/latefusion.png">
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
					<section data-transition="none">
						<br>
						<h3> Hybrid fusion </h3>
						<img height="450" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/hybridfusion.png">
						<ul>
							<li class="question">Where can we merge different data representations?</li>
						</ul>
					</section>
				</section>

				<section data-state="is-in-background">
						<section data-transition="none">
							<aside class="notes">
								The next research problem that we face is
							</aside>
						<h3>Research focus (II/II)</h3>
						<ul>
							<li class="question" style="margin-bottom: 1em">How should we merge data from different sources?</li>
						</ul>
						</section>
						<section data-transition="none">
							<h3>Concatenation (additive conditioning)</h3>
							<figure class="l-body">
    							<svg viewBox="0 0 704 230" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path><path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path><path id="column-top" d="M 0 6 a 6 6 0 0 1 6 -6 l 8 0 a 6 6 0 0 1 6 6 l 0 14 l -20 0 Z"></path><path id="column-middle" d="M 0 0 l 20 0 l 0 20 l -20 0 Z"></path><path id="column-bottom" d="M 0 0 l 20 0 l 0 14 a 6 6 0 0 1 -6 6 l -8 0 a 6 6 0 0 1 -6 -6 Z"></path></defs><text x="260" y="10" dy="1em" class="figure-text"><tspan><tspan style="font-weight: bold;">Concatenation-based conditioning</tspan></tspan><tspan x="260" dy="1.5em"> simply concatenates the conditioning </tspan><tspan x="260" dy="1.5em"> representation to the input. </tspan></text><text x="445" y="100" dy="0.4em" class="figure-text"><tspan> The result is passed </tspan><tspan x="445" dy="1.5em"> through a linear layer </tspan><tspan x="445" dy="1.5em"> to produce the output. </tspan></text><g transform="translate(10, 70)"><text x="0" y="90" dy="0.4em" class="figure-text"> input </text><g transform="translate(40, 60)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use></g><text x="191" y="-30" dy="-0.35em" style="text-anchor: end;" class="figure-text"><tspan>conditioning</tspan><tspan x="191" dy="1.5em">representation</tspan></text><g transform="translate(201, -60)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-film-generator figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 40)" class="figure-film-generator figure-element"></use></g><g transform="translate(191, 30)"><rect width="40" height="120" class="figure-element figure-box"></rect><text x="20" y="60" dy="0.4em" style="text-anchor: middle;" transform="rotate(-90, 20, 60)" class="figure-text">concatenate</text></g><g transform="translate(302, 30)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 80)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 100)" class="figure-filmed-network figure-element"></use></g><g transform="translate(380, 30)"><rect width="40" height="120" class="figure-element figure-box"></rect><text x="20" y="60" dy="0.4em" style="text-anchor: middle;" transform="rotate(-90, 20, 60)" class="figure-text">linear</text></g><g transform="translate(570, 50)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use></g><text x="600" y="90" dy="0.4em" class="figure-text"> output </text><path d="M 60 90 L 186 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(186, 90)" class="figure-path"></use><path d="M 231 90 L 297 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(297, 90)" class="figure-path"></use><path d="M 327 90 L 375 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(375, 90)" class="figure-path"></use><path d="M 420 90 L 565 90" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(565, 90)" class="figure-path"></use><path d="M 211 0 L 211 25" class="figure-line"></path><use xlink:href="#arrow-down" transform="translate(211, 25)" class="figure-path"></use></g></svg>
  							</figure>
							<div class="remark">
								* Image courtesy: <a href="https://distill.pub/2018/feature-wise-transformations/">Dumoulin, et al., "Feature-wise transformations", Distill, 2018. </a>
							</div>
						</section>
						<section data-transition="none">
							<h3>Multiplicative conditioning</h3>
							<figure class="l-body">
								<svg viewBox="0 0 704 300" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path><path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path><path id="column-top" d="M 0 6 a 6 6 0 0 1 6 -6 l 8 0 a 6 6 0 0 1 6 6 l 0 14 l -20 0 Z"></path><path id="column-middle" d="M 0 0 l 20 0 l 0 20 l -20 0 Z"></path><path id="column-bottom" d="M 0 0 l 20 0 l 0 14 a 6 6 0 0 1 -6 6 l -8 0 a 6 6 0 0 1 -6 -6 Z"></path></defs><text x="340" y="30" dy="1em" class="figure-text"><tspan><tspan style="font-weight: bold;">Conditional scaling</tspan> first maps the </tspan><tspan x="340" dy="1.5em"><tspan style="font-weight: bold;">conditioning representation</tspan> to a </tspan><tspan x="340" dy="1.5em"> scaling vector. </tspan></text><text x="340" y="200" dy="1em" class="figure-text"><tspan> The scaling vector is then multiplied </tspan><tspan x="340" dy="1.5em"> with the input. </tspan></text><g transform="translate(-10, 10)"><text x="20" y="250" dy="-0.5em" style="text-anchor: begin;" class="figure-text">input</text><text x="605" y="250" dy="-0.5em" style="text-anchor: begin;" class="figure-text">output</text><g transform="translate(60, 200)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use></g><path d="M 80 240 L 300 240" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(300, 240)" class="figure-path"></use><g transform="translate(315, 240)"><circle r="10" class="figure-element"></circle><ellipse rx="2" ry="2" class="figure-path"></ellipse></g><path d="M 325 240 L 570 240" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(570, 240)" class="figure-path"></use><g transform="translate(575, 200)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-filmed-network figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-filmed-network figure-element"></use></g><text x="20" y="60" dy="-0.5em" style="font-weight: bold;" class="figure-text"><tspan>conditioning</tspan><tspan x="20" dy="1.5em">representation</tspan></text><g transform="translate(170, 0)"><rect width="40" height="120" class="figure-film-generator figure-element figure-box"></rect><text x="20" y="60" dy="0.4em" style="text-anchor: middle;" transform="rotate(-90, 20, 60)" class="figure-text">linear</text></g><g transform="translate(305, 20)"><use xlink:href="#column-top" transform="translate(0, 0)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 20)" class="figure-film-generator figure-element"></use><use xlink:href="#column-middle" transform="translate(0, 40)" class="figure-film-generator figure-element"></use><use xlink:href="#column-bottom" transform="translate(0, 60)" class="figure-film-generator figure-element"></use></g><path d="M 120 60 L 165 60" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(165, 60)" class="figure-path"></use><path d="M 210 60 L 300 60" class="figure-line"></path><use xlink:href="#arrow-right" transform="translate(300, 60)" class="figure-path"></use><path d="M 315 100 L 315 225" class="figure-line"></path><use xlink:href="#arrow-down" transform="translate(315, 225)" class="figure-path"></use></g></svg>
							</figure>
							<div class="remark">
								* Image courtesy: <a href="https://distill.pub/2018/feature-wise-transformations/">Dumoulin, et al., "Feature-wise transformations", Distill, 2018. </a></li>
							</div>
						</section>

						<section data-transition="none">
							<h3>Feature-wise Linear Modulation</h3>
							<figure class="l-body">
								<svg viewBox="0 0 704 390" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path><path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path></defs><text x="0" y="20" class="figure-text"><tspan> The <tspan style="font-weight: bold;">FiLM generator</tspan> processes the conditioning information </tspan><tspan x="0" dy="1.5em"> and produces parameters that describe how the target network </tspan><tspan x="0" dy="1.5em"> should alter its computation. </tspan></text><path d="M420,10l0,510" style="stroke-width: 1px; stroke: #666; opacity: 0.15;"></path><text x="460" y="20" class="figure-text"><tspan> Here, the <tspan style="font-weight: bold;">FiLM-ed network</tspan>â€™s computation </tspan><tspan x="460" dy="1.5em"> is conditioned by two FiLM layers. </tspan></text><g transform="translate(0, 40)"><g transform="translate(460, 40)"><text x="150" y="425" class="figure-text">output</text><path d="M130,399L130,430" class="figure-line"></path><use x="130" y="430" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 337)" class="figure-faded"><rect width="222" height="62" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="31" dy="0.4em" style="text-anchor: middle;" class="figure-text">sub-network</text></g><path d="M130,305.5L130,332" class="figure-line"></path><use x="130" y="332" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 275.5)" class="film-layer"><rect width="222" height="30" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="15" dy="0.4em" style="text-anchor: middle; font-weight: bold;" class="figure-text">FiLM</text></g><path d="M130,244L130,270.5" class="figure-line"></path><use x="130" y="270.5" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 182)" class="figure-faded"><rect width="222" height="62" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="31" dy="0.4em" style="text-anchor: middle;" class="figure-text">sub-network</text></g><path d="M130,149.5L130,177" class="figure-line"></path><use x="130" y="177" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 119.5)" class="film-layer"><rect width="222" height="30" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="15" dy="0.4em" style="text-anchor: middle; font-weight: bold;" class="figure-text">FiLM</text></g><path d="M130,88L130,114.5" class="figure-line"></path><use x="130" y="114.5" xlink:href="#arrow-down" class="figure-path"></use><g transform="translate(20, 26)" class="figure-faded"><rect width="222" height="62" class="figure-filmed-network figure-group figure-box"></rect><text x="111" y="31" dy="0.4em" style="text-anchor: middle;" class="figure-text">sub-network</text></g><text x="150" y="0" class="figure-text">input</text><path d="M130,-15l0,36" class="figure-line"></path><use x="130" y="21" xlink:href="#arrow-down" class="figure-path"></use></g><g transform="translate(0, 190)"><path d="M0,0l110,0" class="figure-line"></path><use x="110" y="0" xlink:href="#arrow-right" class="figure-path"></use><text x="0" y="0" dy="-0.8em" class="figure-text">conditioning</text></g><g transform="translate(115, 140)" class="film-generator"><rect width="180" height="100" class="figure-film-generator figure-group figure-box"></rect><text x="90" y="50" dy="0.4em" style="text-anchor: middle; font-weight: bold;" class="figure-text">FiLM generator</text></g><g><path d="M295,175l175,0" class="figure-line"></path><use x="475" y="175" xlink:href="#arrow-right" class="figure-path"></use><text x="305" y="175" dy="-0.8em" class="figure-text">FiLM parameters</text></g><g><path d="M295,205C360,205,387.5,205,387.5,265S415,330,475,330" class="figure-line"></path><use x="475" y="330" xlink:href="#arrow-right" class="figure-path"></use></g></g></svg>
							</figure>
							<div class="remark">
								* Image courtesy: <a href="https://distill.pub/2018/feature-wise-transformations/">Dumoulin, et al., "Feature-wise transformations", Distill, 2018. </a>
							</div>
						</section>

				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>II - Audio-Visual Musical Instrument Classification</h2>
				</section>

				<section data-state="is-in-classification">
					<aside class="notes">
						Video source: https://www.youtube.com/watch?v=aaxX9Ik6he0 CC0
					</aside>
					<h3> Problem definition </h3>
					<video controls height="200">
						<source src="video/classification_sample.mp4" type="video/mp4">
					</video>
					<img height="80" style="margin-bottom: 50px; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/classification_definition_informal.png">
					<div class="remark" style="text-align: center; margin-top: 2em;">
						* Video courtesy: YouTube-8M dataset, an actual training sample
					</div>


				</section>

				<section data-state="is-in-classification">
					<aside class="notes">
						For the classification setup, we have a set of objects $X \in \mathbb{R}^N$ and a set of class
						labels $C \in \{0,1\}^K$. We are interested in predicting a final subset of
						labels $y=(y_1, y_2, ..., y_K) \in C$ for a sample $x=(x_1, x_2, ..., x_N) \in X$ which is
						an $N$-dimensional representation of an object $x$. Therefore, we are looking for a function such as

						where $\theta$ are the function parameters and $ \hat{y} $ is a probability vector for the
						class estimates. A common mapping function $ f $ could be a complex non-linear neural
						network, where the final probability scores can be obtained via softmax function
						\[ \hat{y_j}(z) = \frac{e^{z_j}}{\sum_{i=1}^L e^{z_i}}, \] which takes a vector of
						arbitrary real-valued scores $z = (z_1, ... z_L) \in \mathbb{R}^L $ and transforms
						it to a vector of values between zero and one that sum to one.

						A common choice to learn the parameters $\theta$ is through a backpropagation
						algorithm where one of the common  optimization techniques can be used to
						minimize a loss function. For multi-labels classification,
						the loss function can be defined as categorical cross-entropy between ground truth and
						estimated probability distributions of class labels:
					</aside>
					<h3> Formal problem definition </h3>

					<ul>
						<li class="fragment"> set of objects $ \definecolor{signal}{RGB}{18,110,213} {\color{signal} X} \subset \mathbb{R}^N$,
							samples $ {\definecolor{signal}{RGB}{18,110,213} \color{signal}x}=({\color{signal} x_1, x_2, ..., x_N}) \in {\color{signal}X}$ </li>
						<li class="fragment"> set of class labels $\definecolor{categories}{RGB}{203,23,206} {\color{categories}C} \subset \{0,1\}^{\color{categories}K}$ </li>
						<li class="fragment"> final subset of labels $ \definecolor{classes}{RGB}{114,0,172}  \definecolor{categories}{RGB}{203,23,206}
							{\color{classes} y } =
							({\color{classes}y_1, y_2, ..., y_K}) \in {\color{categories} C }$ for a sample
							$ \definecolor{signal}{RGB}{18,110,213} \color{signal} x$ </li>
					</ul>
					<p class="fragment">
					\[
					\begin{aligned}
					\definecolor{classes}{RGB}{114,0,172}
					\definecolor{params}{RGB}{45,177,93}
					\definecolor{model}{RGB}{251,0,29}
					\definecolor{signal}{RGB}{18,110,213}
					\definecolor{probability}{RGB}{217,86,16}
					\definecolor{categories}{RGB}{203,23,206}

					{\color{probability} \hat{y}} = {\color{model} f}_{\color{params} \theta}({\color{signal} x})
					\end{aligned}
					\]

						<span style="color: rgb(45,177,93)"> find parameters</span>
						<span style="color: rgb(251,0,29)"> for a model </span> that gives <br>
						<span style="color: rgb(203,23,206)">class </span>
						<span style="color: rgb(217,86,16)">estimation probabilities </span>
						<span style="color: rgb(18,110,213)">for a sample</span>

					</p>

					<p class="fragment">
						\[

						\definecolor{loss}{RGB}{128,121,14}
						\definecolor{classes}{RGB}{114,0,172}
						\definecolor{probability}{RGB}{217,86,16}
						\definecolor{categories}{RGB}{203,23,206}

							{ \color{loss} \mathcal{L} } ( {\color{classes} y}, {\color{probability} \hat{y}})
						= -{\color{loss} \sum}_{\color{categories} i=1}^{\color{categories} K} {\color{classes}y_i} {\color{loss} * \log}( {\color{probability} \hat{y}_i })
						\]

						<span style="color: rgb(203,23,206)">for multi-labels classification, </span>
						minimize <span style="color: rgb(203,23,206)"> categorical </span>
						<span style="color: rgb(128,121,14)"> cross-entropy between </span>
						<span style="color: rgb(114,0,172)"> ground truth </span>
						<span style="color: rgb(128,121,14)"> and </span>
						<span style="color: rgb(217,86,16)"> estimated probability distributions </span>
						<span style="color: rgb(203,23,206)"> of class labels </span>
					</p>


				</section>

				<section data-state="is-in-classification">
					<h3> First multimodal approach </h3>
					<ul>
						<li class="fragment"> Hybrid/Late fusion</li>
						<li class="fragment"> Concatenation</li>
					</ul>
					<div class="fragment">
					<p style="margin: 0; font-size: medium">Audio input
						&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
						&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
						&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
						&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Video input</p>
						<img height="200" style="margin-left: 2em; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/icmr_multimodal.png">
						<p style="margin: 0; font-size: medium">
							Audio-based CNN
							&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
							&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
							&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
							Video-based CNN
						</p>

					</div>

				</section>

				<section data-transition="none" data-state="is-in-classification">
					<h3> First multimodal approach </h3>
					<img height="100" style="margin: 0em; border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/icmr_multimodal.png">

					<div class="multiCol">
						<div class="col fragment">
						<ul>
							<li> MelSpec audio representation </li>
							<li> Architectures
								<ul style="list-style:none">
									<li> Han et al., 1.5M params</li>
									<li> Choi et al., 2.4M params</li>
									<li> Xception, 9.6M params</li>
								</ul>
							</li>
						</ul>
						</div>

						<div class="col fragment">
						<ul>
							<li> Bag-of-frames RGB</li>
							<li>Inception V3</li>
							<li> Number of frames: 10-100
								<ul style="list-style:none">
									<li> FCVID: 20-100 </li>
									<li> YouTube-8M: 10-20 </li>
								</ul>
							</li>
							<li>Fine-tune or train from scratch</li>
						</ul>
						</div>
					</div>

					<blockquote class="fragment" style="margin: 0px auto">
						DATASETS <br>
						<ul>
							<li> FCVID: Musical Performance With Instruments <br> &emsp; 12 classes, 5K videos, 260 hours </li>
							<li> YouTube-8M: MusInstr-Normalized <br> &emsp; 46 classes, 60k videos, 4k hours </li>
						</ul>
					</blockquote>

				</section>

				<section data-state="is-in-classification">
					<section data-transition="none" data-markdown>
						<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | `$ \mathbf{88.27}$` | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | `$87.25$` | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | `$78.95$` | `$+5.21$` | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | `$ \mathbf{84.69}$`| `$-0.43$` | `$-13.6$` |

							 > <!-- .element: class="fragment" --> comparing Xception and Choi et al. as they perform compatible as audio-only models

						</textarea>

					</section>
					<section data-transition="none" data-markdown>
						<aside class="notes">Choi is smaller and have stronger regularization, stronger features </aside>

						<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | `$ \mathbf{88.27}$` | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | `$87.25$` | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | `$78.95$` | <span style="background-color: #c8102e; border-radius: 5px"> &nbsp;`$+5.21$` </span> | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | `$ \mathbf{84.69}$`| `$-0.43$` | `$-13.6$` |

							<span style="background-color: #c8102e; border-radius: 5px"> &nbsp; `${}^*$` Issues with YouTube-8M annotations &nbsp; </span> <!-- .element: class="fragment" -->
							![piano_errors](img/piano2.png) <!-- .element: class="fragment" -->
						</textarea>
					</section>
					<section data-transition="none" data-markdown>
						<aside class="notes">Choi is smaller and have stronger regularization, stronger features </aside>

							<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | `$ \mathbf{88.27}$` | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | `$87.25$` | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | `$78.95$` | <span style="background-color: #c8102e; border-radius: 5px"> &nbsp;`$+5.21$` </span> | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | `$ \mathbf{84.69}$`| `$-0.43$` | `$-13.6$` |

							<span style="background-color: #c8102e; border-radius: 5px"> &nbsp; `${}^*$` Issues with YouTube-8M annotations &nbsp; </span>
							![piano_errors](img/piano3.png)
							</textarea>
					</section>
					<section data-transition="none" data-markdown>
						<aside class="notes">Choi is smaller and have stronger regularization, stronger features </aside>

							<textarea data-template>
							### Summary of results

							| Method | Dataset | F1 | `$ \Delta $` F1-A | `$ \Delta $` F1-V |
							| :----- | -------- | -- | ---- | ----- |
							|Xception / 50 frames | FCVID | `$ \mathbf{88.27}$` | `$-8.92$` | `$-17.04$` |
							|Choi et al. / 50 frames | FCVID | `$87.25$` | `$-8.54$`| `$-16.02$` |
							|Xception / 20 frames | YT-8M | `$78.95$` | <span style="background-color: #c8102e; border-radius: 5px"> &nbsp;`$+5.21$` </span> | `$-7.86$` |
							|Choi et al. / 20 frames | YT-8M | `$ \mathbf{84.69}$`| `$-0.43$` | `$-13.6$` |

							<span style="background-color: #c8102e; border-radius: 5px"> &nbsp; `${}^*$` Issues with YouTube-8M annotations &nbsp; </span>
							![piano_errors](img/piano4.png)
							</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### Audio-only results
							|Method | Params | Dataset | Hit@1 | Hit@3 | F1 |
							| :----- | -------- | --- | ---- | ----- | --- |
							| Han et al. | 1.5M |  FCVID  |  64.13 | 76.82 | 53.64 |
							| Choi et al. + CC | 2.4M |  FCVID |  77.73 | 92.05 | 77.18 |
							| Choi et al. + UC | 2.4M |  FCVID |  **79.81** | **96.09** | 78.71 |
							| Xception + UC | 9.6M |  FCVID | 78.69 | 94.44 | **79.35** |
							| Han et al. | 1.5M | YT-8M | 59.37 | 70.87 | 56.50 |
							| Choi et al. + UC | 2.4M | YT-8M | **83.58** | 94.23 | **84.26** |
							| Xception + UC | 9.6M | YT-8M | 83.53 | **94.69** | 84.16 |
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Visual-only results

							|Dataset | FMs | PT | Steps | Time | Hit@1 | Hit@3 | F1 |
							| :----- | --- | --- | --- | ---- | ----- | ------ | --- |
							|FCVID  | 20 | No | 32K | 19h | 42.30 | 64.53 | 43.16 |
							|FCVID  | 30 | No | 16K | 11h | 65.39 | 81.75 | 67.29 |
							|FCVID  | 30 | Yes | 16K | 11h | 68.77 | 84.26 | 70.33 |
							|FCVID  | 50 | No | 24K | 22h | 67.47 | 83.21 | 69.38 |
							|FCVID  | 50 | Yes | 21K | 19h | **69.39** | **84.32** | **71.23** |
							|FCVID  | 100 | No | 43K | 98h | 68.56 | 83.97 | 70.42 |
							|FCVID  | 100 | Yes | 36K | 84h | 67.76 | 83.50 | 69.16 |
							|YT-8M  | 10 | No | 58K | 82h | 61.15 | 78.45 | 52.19 |
							|YT-8M  | 20 | Yes | 57K | 92h | **70.07** | **84.20** | **71.09** |
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Audio-visual results
							| Method | Dataset | Hit@1 | Hit@3 | F1 |
							| :----- | -------- | --- | ---- | ----- |
							|Xception @ 50 frames | FCVID | **88.28** | **97.00** | **88.27** |
							|Choi et al. @ 50 frames | FCVID | 86.97 | 96.09 | 87.25 | 
							|Xception @ 20 frames | YT-8M | 82.64 | 91.37 | 78.95 |
							|Choi et al. @ 20 frames | YT-8M | **84.01** | **93.41** | **84.69** |
						</textarea>
					</section>
				</section>

				<section data-state="is-in-classification">
					<h3>Summary of contributions</h3>
					<ul>
						<li> Proposed a new multimodal network for musical instrument classification</li>
						<li> Demonstrated efficiency of the audio-visual approach </li>
						<li> Shown that multi-modal fusion is beneficial overall</li>
						<li> Concatenation might not be enough for controversial inputs </li>
					</ul>
				</section>

				<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2>III - Audio-Visual Source Separation</h2>
				</section>

				<section data-state="is-in-separation">
					<h2>Can you hear the difference?</h2>
					<br>
					<p>&nbsp;</p>
					<p>&nbsp;</p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_1.wav" > <span class="music">Sample 1</span> </p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_2.wav" > <span class="music">Sample 2</span></p>
					<p class ="fragment" data-audio-src="audio/rondeau_example_3.wav" > <span class="music">Sample 3</span></p>
					<p>&nbsp;</p>
					<p>&nbsp;</p>
					<div class="remark">
						* All examples are taken from the URMP dataset: <a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a>
					</div>

					<aside class="notes">
						To introduce the topic I'm going to let you hear three short excerpts of the same composition
						(the same musical piece) and ask you a simple question.
						I believe that most you can hear a significant difference between sample 3 and the first two
						examples but... what it's much harder in case of sample 1 and sample 2.
						how many could hear the difference? The overlap is so strong that it's almoust impossible without
						special training.
						Now, I would like you to hear the actual difference between the three of them.
					</aside>
				</section>

				<section data-state="is-in-separation">
					<p>Can you hear the difference <span style="color: orange">now?</span></p>
					<br>

					<aside class="notes"> 
						This is how the first mixture is different from the second one.
						This is how the second mixture is different from the first one.
						This is residue of the third mixture with respect to the first two.

						Can you say if the first samples are from the same musical instrument or not?
						Would it be easier for you to distinguish between two recordings if I could provide you more information.... like this?
					</aside>

					<table>
						<thead>
							<tr>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_1_db.wav" >
									<span class="music">Separated 1</span>
								</span></th>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_2_vc.wav" >
									<span class="music">Separated 2</span>
								</span></th>
								<th><span class ="fragment" data-audio-src="audio/rondeau_example_3_cl.wav" >
									<span class="music"> Separated 3 </span>
								</span></th>
							</tr>
						</thead>
						<tbody>
							<tr class ="fragment">
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_1_db.gif"></td>
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_2_vc.gif"></td>
								<td style="text-align: right"><img height="150" data-src="video/rondeau_example_3_cl.gif"></td>
							</tr>
						</tbody>
					</table>

				</section>

				<section data-state="is-in-separation">
					<aside class="notes"> 
						Just keep it in mind for a little, while I'm continue to introduce the task which we're trying to solve.
						Having a single-channel recording of a musical performance, that is to say a mixture of many possibly overlapping signals,
						we want to estimate their individual tracks, their sources.
						It can be used for hearing aid, music production, music education, as a preprocessing for other
						analysis tasks and so on.
						Here is an illustration of our goal.  
					</aside>

						<h3> Problem definition </h3>
						<video controls>
							<source src="video/vimss_idea_demo.mp4" type="video/mp4">
						</video>
				</section>

				<section data-state="is-in-separation">
					<aside class="notes">
					Single channel source separation (SCSS) consists of estimating the individual sources
						x_i given a mono mixture time-domain signal $y$ of $N$ sources:

					Instead of predicting time-domain signals, a general approach for solving SCSS involves the
						estimation of N masks for Short-Term Fourier transform (STFT) values of the mixture.
						In this case, we consider a time-frequency representation of the mixture Y
						and the  sources Xi, and the goal of the source separation method is to
						learn a real-valued (or complex-valued) mask $M_i$ for each source $i$ .

					Let us denote by $|\boldsymbol{X_i}(\tau, \omega)|$ and $|\boldsymbol{Y}(\tau, \omega)|$
						the magnitude of the STFT value, of $\boldsymbol{X_i}$ and $\boldsymbol{Y}$ respectively,
						at frequency $\omega$ and time frame $\tau$.
					In this work, we only consider two types of \textit{real-valued} masks,
						namely \textit{ideal ratio or soft} masks $M_i^{ir}$:

					</aside>
					<section data-transition="none">
					<h3> Formal problem definition </h3>

					<ul>
						<li class="fragment"> a mono mixture time-domain signal $\definecolor{mix}{RGB}{18,110,213} \color{mix} y(t)$ </li>
						<li class="fragment"> individual sources $ \definecolor{gt}{RGB}{203,23,206}  \color{gt} x_i(t)$ </li>
					</ul>
					<p class="fragment">
							\[
								\definecolor{loss}{RGB}{128,121,14}
								\definecolor{predicted}{RGB}{217,86,16}
								\definecolor{gt}{RGB}{203,23,206}
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{sum}{RGB}{251,0,29}
								\definecolor{mix}{RGB}{18,110,213}

								{\color{mix} y(t)} = {\color{sum} \sum_{\color{nsources}i=1}^{\color{nsources}N} {\color{gt} x_i(t)}}

							\]

							<span style="color: rgb(18,110,213)"> the mixture </span>
							equals <span style="color: rgb(251,0,29)">to the sum of </span>
							<span style="color: rgb(114,0,172)"> all </span>
							<span style="color: rgb(203,23,206)"> sources </span>
					</p>
					<p class="fragment">
							\[
								\definecolor{loss}{RGB}{128,121,14}
								\definecolor{predicted}{RGB}{217,86,16}
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{model}{RGB}{251,0,29}
								\definecolor{gt}{RGB}{203,23,206}
								\definecolor{mix}{RGB}{18,110,213}
								\definecolor{param}{RGB}{45,177,93}

								{\color{predicted} \hat{x}_{\color{nsources}i}(t)} =
									{\color{model}f_{\color{params}\theta}^{\color{nsources}i}}({\color{mix}y(t)})

							\]

							<span style="color: rgb(45,177,93)"> find parameters</span>
							<span style="color: rgb(251,0,29)"> for a model </span> that gives <br>
							<span style="color: rgb(114,0,172)"> individual </span>
							<span style="color: rgb(217,86,16)"> sources estimation </span>
							<span style="color: rgb(18,110,213)">from the mixture</span>
						</p>
					</section>

					<section data-transition="none">
						<h3> Formal problem definition </h3>
						<p>approaches to estimate individual sources from the mixture</p>
						<div class="multiCol">

						<div class="col fragment" style="text-align: center">
							<h4> direct waveform estimation</h4>
							<p>predicting time-domain signals $
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{predicted}{RGB}{217,86,16}
								\color{predicted} \hat{x}_{\color{nsources}i}(t)$</p>
							<p class="fragment">
							\[
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{loss}{RGB}{128,121,14}
								\definecolor{predicted}{RGB}{217,86,16}
								\definecolor{gt}{RGB}{203,23,206}

							{\color{loss} \mathcal{L}^{w} {\color{black}=} \sum_{\color{nsources}i=1}^{\color{nsources}N}
								\sum_{j=1}^{T} ({\color{gt} x_{\color{nsources}i}(j)} -
								{\color{predicted} \hat{x}_{\color{nsources}i}(j)})^2} \]

							</p>
						</div>
						<div class="col fragment" style="text-align: center">
							<h4>masking-based approach</h4>
							<p>predicting ratio masks $
								\definecolor{nsources}{RGB}{114,0,172}
								\definecolor{predicted}{RGB}{217,86,16}
								\color{predicted} \hat{M}_{\color{nsources}i}^{r} $</p>
							<p> <span style="font-size: medium"> $ \definecolor{mix}{RGB}{18,110,213} \color{mix}
								\boldsymbol{Y} $,
								$ \definecolor{gt}{RGB}{203,23,206}
								  \definecolor{nsources}{RGB}{114,0,172} \color{gt}
									\boldsymbol{X_{\color{nsources} i}} $
									STFT values of the mixture and sources </span> </p>

							<p>	<span style="font-size: medium"> magnitude of the STFT value at frequency $\nu$ and time $\tau$ <br>
								$\definecolor{gt}{RGB}{203,23,206}
								 \definecolor{nsources}{RGB}{114,0,172}
								| {\color{gt} \boldsymbol{X_{\color{nsources}i}}(\tau, \nu) } |$,
								$\definecolor{mix}{RGB}{18,110,213}
								| {\color{mix} \boldsymbol{Y}(\tau, \nu)}|$ </span>
							</p>
							<p>
								<span style="font-size: medium"> ideal ratio mask of source $\definecolor{gt}{RGB}{203,23,206}
								 \definecolor{nsources}{RGB}{114,0,172}
								| {\color{gt} \boldsymbol{X_{\color{nsources}i}}(\tau, \nu) } |$

								\[	\definecolor{gt}{RGB}{203,23,206}
									\definecolor{nsources}{RGB}{114,0,172}
									\definecolor{mix}{RGB}{18,110,213}
									\definecolor{predicted}{RGB}{217,86,16}

									{\color{predicted} \hat{M}_{\color{nsources}i}^{r}} =

									\frac{|{\color{gt} \boldsymbol{X_{\color{nsources}i}}(\tau, \nu) } |}{|{\color{mix} \boldsymbol{Y}(\tau, \nu)}|}
								\]</span>
							</p>

							<p class="fragment">
							\[

							\definecolor{loss}{RGB}{128,121,14}
							\definecolor{nsources}{RGB}{114,0,172}
							\definecolor{predicted}{RGB}{217,86,16}
							\definecolor{gt}{RGB}{203,23,206}

							{\color{loss} \mathcal{L}^{r} {\color{black}=} \sum_{\color{nsources}i=1}^{\color{nsources}N} \sum_{j=1}^{|(\tau, \nu)|}
								({\color{gt} M_{\color{nsources}i}^{ir}(j)} -
								{\color{predicted} \hat{M}_{\color{nsources}i}^{r}(j)})^2} \]

						</p>

						</div>
					</div>

					</section>
					<section data-transition="none">
						<h3>Metrics</h3>
						<ul>
							<li class="fragment">Source to Distortion Ratio (SDR)</li>
							<li class="fragment">Source to Inferences Ratio (SIR)</li>
							<li class="fragment">Sources to Artifacts Ratio (SAR)</li>
							<li class="fragment">Scale-Independent Source to Distortion Ratio (SI-SIR)</li>
							<li class="fragment">Scale-Dependent Source to Distortion Ratio (SD-SIR)</li>
							<li class="fragment">Predicted Energy at Silence (PES)</li>

						</ul>

					</section>

				</section>

				<section data-state="is-in-separation">
					<aside class="notes"> 
						Now I would like to shortly discuss the common approach to solve this problem and why it is difficult to solve.
						One of the problems is that sources often overlap and time and frequency. It's especially true for instruments playing in harmony and in unison, this is often the case for classical music. And this is also a problem for instruments of the same family, since they have very similar timbre as well. 
						
						It has been shown that the complexity of the task increases with the number of sources, and in our work we tackle the problem not of just two-channel source separation, but we address multiple instrument separation where actual number of sources is not known.

						For many years standard common approach for this task had consisted in estimating binary or ratio masks which could be applied to a magnitude of mixture spectrogram in order to obtain the magnitude spectrogram of the sources, and then recostruct the original sound using either the mixture phase of phase-reconstruction algorithm.
						Thus we lose some information which can be potentially usefull for source separation and this is our primary motivation for waveform domain source separation.

						On the other hand, with all this complexity and the first example I've shown you in mind, we want to answer another question:

						So we took week information, which is instrument labels information which can be available from another classifier network from taken from a different modality, a decided to use them to condition our model.  

					</aside>

					<section>
						<h2>Known challenges</h2>
						<ul>
							<li class="fragment">Same-family instruments are similar to one another</li>
							<li class="fragment">Unknown in advance number of sources</li>
							<li class="fragment">Complexity increases with the number of sources</li>
							<li class="fragment">Overlap in time and frequency between sources</li>
							<!--
							<li class="fragment">Extra: inter-family mimicry for some instruments (clarinet vs. viola)</li>
							-->
						</ul>
						<br>
						<br>
						<p class="fragment" style="width: 80%; margin: auto; background-color: hsl(11,65%,70%); padding: 20px 0px 20px 10px; color:white">
							<span style="font-size:48px; float: left">ðŸ’ª</span>
							<span style="font-size:48px">Can we use extra information to improve separation? </span>
						</p>
					</section>
				</section>

				<section data-transition="none" data-state="is-in-separation">
					<aside class="notes"> 
						Let me briefly explain the architecture.
						Overall, in deep learning based source separation, there is a huge variety of architecture design. 
						All that start from an encoder-decoder architecture (but STFT-based), when people started to use U-net with skip connections but also STFT-based,
						then Wave-U-Net appeared, and we inhereted Wave-U-Net model from their authors. 
						But, first of all, we are trying to estimated multiple sources which are very sparce, and, finally,
						we apply a feature modulation with instrument labels on the bottleneck of the U-Net.

						It's multiplicative conditioning, because back then the authors weren't aware of FiLM. We only applied it to the bottleneck although later we tried other combinations.   
					</aside>

					<section data-transition="none">
						<h3>Preliminary study</h3>
						<ul>
							<li class="fragment">Multi-instrument Source Separation in Time Domain</li>
							<li class="fragment">Hybrid fusion</li>
							<li class="fragment">Multiplicative Conditioning on Instrument Labels</li>
						</ul>
					</section>


					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
							<img class="fragment" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net1.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net2.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net3.png">
					</section>
					<section data-transition="none"> 
						<h3> Conditioned Wave-U-Net Architecture </h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/CExp-U-Net4.png">
					</section>
				</section>

				<section data-state="is-in-separation">
					<aside class="notes"> 
						This was trained with a simple MSE loss between original and reconstructed sources, it was trained on a multi-modal dataset of music performances from Rochester university, which is relatively small and has its issues. 
						So, just a disclaimer, the results which I'm going to show you now are results of the network which has been trained with 30 videos which is about 1,5h in total.
					</aside>
					<section>
						<h3> Experiments </h3>
					</section>

					<section data-background-image="img/creation_process.png" data-background-opacity="0.2" data-background-size="contain">
						<h4 class="fragment" data-fragment-index="1">URMP <a href="https://doi.org/10.5061/dryad.ng3r749">Dataset</a></h4>
						<ul class="fragment" data-fragment-index="2" style="background-color: rgba(255, 255, 255, 0.7)">
							<li class="fragment" data-fragment-index="2">13 instruments</li>
							<li class="fragment">44 videos -> 40 valid pieces </li>
							<li class="fragment">12 duest, 20 trios, 8 quartets </li>
							<li class="fragment">87 unique audio tracks </li>
							<li class="fragment">Train/Test split: 30/10 pieces </li>
						</ul>

					</section>

					<section>
						<h3>Video Demo: Trained on 30 videos</h3>
						<video controls>
							<source src="video/urmp_demo_multisource.mp4" type="video/mp4">
						</video>
					</section>

					<section>
						<aside class="notes"> 
							We compared our method with InformedNMF approch, which was the closest method able to
							separate sounds of different instruments and it uses pretrained timbre models. And,
							surprisingly, our method performs better in SIR.
						</aside>

						<h3>Summary of results</h3>
						<table>
							<thead>
								<tr>
									<th>Method</th>
									<th>SDR</th>
									<th>SIR</th>
									<th>SAR</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>InformedNMF</td>
									<th>-0.16</th>
									<th style="font-weight: normal;">1.42</th>
									<th style="font-weight: normal;">9.31</th>
								</tr>
								<tr>
									<td>Exp-Wave-U-Net</td>
									<th style="font-weight: normal;">-4.12</th>
									<th style="font-weight: normal;">-3.06</th>
									<th>12.18</th>
								</tr>
								<tr>
									<td>CExp-Wave-U-Net</td>
									<th style="font-weight: normal;">-1.37</th>
									<th>2.16</th>
									<th style="font-weight: normal;">6.36</th>
								</tr>
							</tbody>
						</table>
					</section>

				<section>

					<aside class="notes"> 
						But we achieve the most notable difference with the baseline when the complexity of the problem increases. 
						Here you can see that InformedNMF outperforms our model when we test task is the separation of two instruments, but as the number of sources increases, the conditioned wave-u-net notably outperforms the baseline.
					</aside>

					<h3>Summary of results</h3>
					<img data-src="img/sdr.png" height="300px" style="border:1px solid #555a5f">
					<img data-src="img/sir.png" height="300px" style="border:1px solid #555a5f">
					<!--<img data-src="img/sar.png" height="250px" style="border:1px solid #555a5f">-->
				</section>

				<section>

					<aside class="notes">
						Take away note: it was inspiring but we lacked TPUs and scaling was not an option
					</aside>

					<h3>Summary of results</h3>
					<blockquote>Take away note: it was inspiring but we lacked TPUs and scaling was not an option
					</blockquote>
				</section>

				<section>

					<aside class="notes"> 
						As I said, we took the original Wave-U-Net model and did quite a couple of changes. 
						For example, we optimized the learning rate, we optimized data loading pipeline, we ported it such that it was possible to train on GC TPUs (which we were able to use at the time), also tested half-precision, which resulted in 35 times faster training comparing to the original model. 
					</aside>

					<h3> Ablation and Speedup</h3>
					<p class="fragment">Learning rate</p>
					<p class="fragment">TPUs</p>
					<p class="fragment">tf.float32 vs tf.float16</p>
					<p class="fragment" style="font-size: larger; color: orange">Total Speedup: x35.4</p>
				</section>
			</section>

			<section data-state="is-in-separation">
				<p>What we did next...</p>

				<!--
				<p class="fragment">
					<a style="font-size: smaller;" href="https://veleslavia.github.io/conditioned-u-net/">https://veleslavia.github.io/conditioned-u-net/</a>
				</p>
				-->
			</section>

			<section data-background-image="img/spreadsheet.png" data-background-opacity="0.2" data-background-size="contain" data-state="is-in-separation with-background">
				<h3 style="background-color: rgba(255, 255, 255, 0.7); margin-bottom: 0em;"> SOLOS </h3>
				<div class="remark" style="text-align: center; background-color: rgba(255, 255, 255, 1); margin-bottom: 2em;">
					* Joint work with Juan Montesinos <a href="https://juanmontesinos.com/Solos">https://juanmontesinos.com/Solos</a>
				</div>

				<ul>
					<li class="fragment" data-fragment-index="1" style="background-color: rgba(255, 255, 255, 1)">13 instruments matching the URMP dataset</li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 1)">Only solo performances / auditions </li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 1)">Semi-automatic and manual quality control </li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 1)">Mix-and-separate approach</li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 1)">755 individual recordings </li>
					<li class="fragment" style="background-color: rgba(255, 255, 255, 1)">Extra: valid timestamps and skeletons  </li>
				</ul>

				<p>&nbsp;</p>
				<p>&nbsp;</p>

			</section>

			<section data-state="is-in-separation">
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3 class="fragment" >Design Decisions</h3>
						<img class="fragment" style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params.svg">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_baseline.svg">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_final.svg">
				</section>
				<section data-transition="none">
					<h3> Conditioned <strike>Wave-</strike>U-Net Architecture </h3>
					<h3>Design Decisions</h3>
						<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_final_data.svg">
				</section>

				<section data-transition="none">
					<h3>Context vectors</h3>
					<div class="multiCol">
						<div class="col fragment">
							<h5>Label conditioning</h5>
							<img data-src="img/cv_binary.png" height="300px" style="border:1px solid #555a5f">
						</div>
						<div class="col fragment">
							<h5>Visual conditioning</h5>
							<img data-src="img/cv_visual.png" height="300px" style="border:1px solid #555a5f">
						</div>
						<div class="col fragment">
							<h5>Visual-motion conditioning</h5>
							<img data-src="img/cv_motion.png" height="300px" style="border:1px solid #555a5f">
						</div>
					</div>
				</section>

				<section data-transition="none">
					<h3>Design Decisions</h3>
					<img style="border:0; box-shadow: 0px 0px 0px #ffffff;" data-src="img/params_final_conditioning.svg">
				</section>

				<section data-transition="none">
					<h3>Conditioning Design</h3>
					<div class="multiCol">
						<div class="col fragment">
							<h5>FiLM conditioning</h5>
							<img height="300px" style="border:0; box-shadow: 0px 0px 0px #ffffff; " data-src="img/cunet_conditioning_film.png">
						</div>
						<div class="col fragment">
							<h5>Multiplicative conditioning</h5>
							<img height="300px" style="border:0; box-shadow: 0px 0px 0px #ffffff; " data-src="img/cunet_conditioning_multiply.png">
						</div>
					</div>

				</section>

			</section>

			<section data-state="is-in-separation">
				<section data-transition="none">
					<h3>Results: Video Demo</h3>
					<video controls>
						<source src="video/C-U-Net_Demo.mp4" type="video/mp4">
					</video>
				</section>

					<section data-transition="none" data-markdown style="font-size: medium">
						<textarea data-template>
							### Summary of results

							|Method | ID | Cond | SI-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
							| :--- | :-----: | :----: | :----: | :----: |  :----: |  :---: |  :---: |
							| U-Net | 6 | none | `$  -2.1\pm 6.2 $` | `$ -8.1\pm 5.9 $` | `$ -0.77\pm5.76 $` | `$ 0.57\pm6.85 $` | `$ 10.88\pm3.21 $` |
							| U-Net | 6-W`$_2$` | none | `$ -7.4\pm 9.9 $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ 1.46\pm8.39 $` | `$ 5.74\pm5.23 $` |
							| FiLM-encoder | 9-W`$_2$` | labels | `$ \mathbf{-3.5\pm6.1} $` | `$ -4.4\pm6.8 $`  | `$ \mathbf{-1.68\pm5.47} $` | `$ -0.39\pm6.48 $` | `$ \mathbf{11.44\pm4.15} $` |
							| FiLM-final | 10-W`$_2$` | labels | `$ -10.1\pm11.7 $` | `$ \mathbf{-33.9\pm28.6 }$` | `$ -3.50\pm6.79 $` | `$ 0.29\pm9.14 $` | `$ 5.88\pm4.97 $` |
							|FiLM-bottleneck-1F | 17-W`$_2$` | visual | `$ -8.7\pm10.8 $` | `$ \mathbf{-25.9\pm23.9} $` | `$ -3.02\pm7.10 $` | <span style="color:gray">`$ 1.17\pm8.95 $`</span> | <span style="color:gray">`$ 5.09\pm5.08 $`</span> |
							|Final-multiply-1F | 16-W`$_2$` | visual | `${-4.1\pm7.2}$` | `$ -15.4\pm14.3 $` | `$ \mathbf{-1.49\pm6.08} $` | `$ 0.98\pm8.39 $` | `$ 9.18\pm3.71 $` |
							|FiLM-final-lstm-5F | 13-W`$_2$` | visual-motion | `$ \mathbf{-3.8\pm6.3} $` | `$ -9.4\pm12.2 $` | `$ -2.51\pm5.32 $` | `$ -1.43\pm6.38 $` | `$ \mathbf{12.44\pm4.72} $` |
							|SoP-unet5-Solos-3F | 21 | visual| `$-16.9\pm8.6$` | n/a | `$-2.9\pm4.7$` | `$-1.67\pm5.34$` | `$11.07\pm6.87$` |
						</textarea>
					</section>

				<!--
								|Method | ID | Cond | SI-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
								| :--- | :-----: | :----: | :----: | :----: |  :----: |  :---: |  :---: |
								| FiLM-bottleneck | 8-W`$_2$` | labels | `$ -8.1\pm9.4 $` | `$ -19.5\pm15.3 $` | `$ -2.77\pm6.64 $` | <span style="color:gray">`$ 1.17\pm8.24 $`</span> | <span style="color:gray">`$ 4.95\pm4.84 $`</span> |
								| FiLM-encoder | 9-W`$_2$` | labels | `$ \mathbf{-3.5\pm6.1} $` | `$ -4.4\pm6.8 $`  | `$ \mathbf{-1.68\pm5.47} $` | `$ -0.39\pm6.48 $` | `$ \mathbf{11.44\pm4.15} $` |
								| FiLM-final | 10-W`$_2$` | labels | `$ -10.1\pm11.7 $` | `$ \mathbf{-33.9\pm28.6 }$` | `$ -3.50\pm6.79 $` | `$ 0.29\pm9.14 $` | `$ 5.88\pm4.97 $` |
								|FiLM-encoder-1F | 12-W`$_2$` | visual | `$ -8.3\pm8.9 $` | `$ -12.2\pm13.4 $` | `$ -3.38\pm5.54 $` | `$ -0.69\pm6.60 $` | `$ 5.91\pm4.25 $`  |
								|FiLM-bottleneck-1F | 17-W`$_2$` | visual | `$ -8.7\pm10.8 $` | `$ \mathbf{-25.9\pm23.9} $` | `$ -3.02\pm7.10 $` | <span style="color:gray">`$ 1.17\pm8.95 $`</span> | <span style="color:gray">`$ 5.09\pm5.08 $`</span> |
								|FiLM-final-1F | 18-W`$_2$` | visual | `$ -4.8\pm7.4 $` | `$ -10.9\pm8.7 $` | <span style="color:gray">`$ -2.24\pm6.22 $`</span> | `$ 0.05\pm7.91 $` | `$ 8.87\pm4.90 $` |
								|Final-multiply-1F | 16-W`$_2$` | visual | `${-4.1\pm7.2}$` | `$ -15.4\pm14.3 $` | `$ \mathbf{-1.49\pm6.08} $` | `$ 0.98\pm8.39 $` | `$ 9.18\pm3.71 $` |
								|FiLM-final-lstm-5F | 13-W`$_2$` | visual-motion | `$ \mathbf{-3.8\pm6.3} $` | `$ -9.4\pm12.2 $` | `$ -2.51\pm5.32 $` | `$ -1.43\pm6.38 $` | `$ \mathbf{12.44\pm4.72} $` |
								|FiLM-bottleneck-lstm-5F | 14-W`$_2$` | visual-motion | `$ -4.7\pm6.9 $` | `$ -10.3\pm12.7 $` | `$ -1.55\pm5.94 $` | `$ 1.02\pm7.96 $` | `$ 7.90\pm3.27 $` |
								|FiLM-bottleneck-mp-5F | 15-W`$_2$` | visual | `$ -7.0\pm7.8 $` | `$ -12.1\pm15.0 $` | `$ -2.46\pm5.82 $` | `$ 0.54\pm7.66 $` | `$ 6.28\pm3.61 $` |
								|SoP-unet7-ft-3F | 20 | visual | `$-17.5\pm8.5$` | n/a | `$-2.57\pm4.99$` | `$0.47\pm6.43$` | `$6.89\pm2.48$` |
								|SoP-unet5-Solos-3F | 21 | visual| `$-16.9\pm8.6$` | n/a | `$-2.9\pm4.7$` | `$-1.67\pm5.34$` | `$11.07\pm6.87$` |
				-->
				<section data-transition="none">
					<h3>Summary of results</h3>
					<img height="400px" style="border:0; box-shadow: 0px 0px 0px #ffffff; " data-src="img/taslp_new_pics/exp69_dtq.png">
				</section>

				<section data-transition="none">
					<h3>Summary of results</h3>
					<img height="400px" style="border:0; box-shadow: 0px 0px 0px #ffffff; " data-src="img/taslp_new_pics/per_instrument_metrics.png">
				</section>

				<section data-markdown style="font-size: medium">
					<textarea data-template>
					## Ablation study results

					|Method | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
					| :--- | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
					| IRM | U | `$ 13.1\pm5.4 $` | `$ 12.7\pm6.4 $`  | n/a | `$ 11.79\pm4.26 $` | `$ 19.94\pm5.59 $` | `$ 13.02\pm4.25 $` |
					| input mix | L | `$-3.7\pm5.7$` | `$-3.7\pm5.7$` | `$18.2\pm4.2$` | `$ -3.48\pm4.82 $` | `$ -3.20\pm4.95 $` | `$ 18.10\pm11.21 $` |
					| No filtering | 1 | `$  -2.1\pm 6.2 $` | `$ -15.2\pm 10.0 $` | `$ -8.1\pm 5.9 $` | `$ -0.77\pm5.76 $` | `$ 0.57\pm6.85 $` | `$ 10.88\pm3.21 $` |
					| Wiener (1 it.) | 1-W`$_1$` | `$ -3.9\pm 7.5 $` | `$ -16.0\pm 16.1 $` | `$ -15.1\pm 10.1 $` | `$ -1.01\pm6.20 $` | `$ 1.53\pm7.65 $` | `$ 7.70\pm3.81 $` |
					| Wiener (2 it.) | 1-W`$_2$` | `$ -7.6\pm 10.7 $` | `$ -21.4\pm 24.1 $` | `$ -25.4\pm 16.8 $` | `$ -2.56\pm7.10 $` | `$ \mathbf{1.72\pm8.52} $` | `$ 4.99\pm5.62 $` |
					| 1-W`$_2$` with CL | 2-W`$_2$` | `$ -8.2\pm 10.9 $` |  `$ -24.1\pm 25.1  $` |  `$ -26.2\pm 17.8  $` |  `$ -3.06\pm7.06  $` | <span style="color:gray">`$ 1.38\pm8.55 $`</span> | <span style="color:gray">`$ 4.59\pm5.72 $`</span>  |
					| 2-W`$_2$` but binary masks | 3-W`$_2$` |  <span style="color:gray">`$ -9.7\pm 15.2 $`</span> | <span style="color:gray">`$ -21.6\pm 27.4 $`</span> |  `$ \mathbf{-36.2\pm 18.4}  $` |  `$ -3.82\pm7.95  $` |  `$ 0.09\pm7.10  $` |  `$ 4.93\pm7.53  $`  |
					| 1-W`$_2$` with noise | 4-W`$_2$` |  <span style="color:gray">`$ -7.6\pm 9.2 $`</span> |  `$-21.7\pm 20.4  $` |  `$ -21.7\pm 15.6 $` |  `$ -3.12\pm6.91  $` |  `$ 0.21\pm8.34  $` |  `$ \mathbf{6.04\pm5.32} $` |
					| 1-W`$_2$` no dB normalise | 5-W`$_2$` | `$ -10.1\pm 11.6 $` |  `$ -25.3\pm 24.9  $` |  `$ -27.0\pm 17.0  $` |  `$ -3.88\pm7.63  $` |  `$ 0.70\pm9.45  $` |  `$ 4.63\pm6.13  $` |
					| 1-W`$_2$` linear-scale STFT | 6-W`$_2$` |  <span style="color:gray">`$\mathbf{-7.4\pm 9.9} $`</span> |  `$ \mathbf{-17.5\pm 19.8} $` |  `$ -24.4\pm 15.8  $` | <span style="color:gray">`$\mathbf{-2.27\pm6.85}$`</span> | <span style="color:gray">`$ 1.46\pm8.39 $`</span> | `$ 5.74\pm5.23 $` |
					| 1-W`$_2$` but MHU-Net  | 7-W`$_2$` | `$ -8.1\pm9.1 $` | `$ -23.7\pm20.4 $` | `$ -20.9\pm15.8 $` | `$ -3.29\pm6.35 $` |  `$ 0.07\pm8.22 $` |  `$ 5.75\pm4.53 $` |
					</textarea>
				</section>

				<section data-markdown  style="font-size: medium">
					<textarea data-template>
					## Label conditioning results

					|Method | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
					| :--- | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
					| w/o conditioning | 6-W`$_2$` | `$ -7.4\pm 9.9 $` | `$ -17.5\pm 19.8 $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ 1.46\pm8.39 $` | `$ 5.74\pm5.23 $` |
					| FiLM-bottleneck | 8-W`$_2$` | `$ -8.1\pm9.4 $` | `$ -19.1\pm18.1 $` | `$ -19.5\pm15.3 $` | `$ -2.77\pm6.64 $` | <span style="color:gray">`$ 1.17\pm8.24 $`</span> | <span style="color:gray">`$ 4.95\pm4.84 $`</span> |
					| FiLM-encoder | 9-W`$_2$` | `$ \mathbf{-3.5\pm6.1} $` | `$ -20.7\pm6.7 $` | `$ -4.4\pm6.8 $`  | `$ \mathbf{-1.68\pm5.47} $` | `$ -0.39\pm6.48 $` | `$ \mathbf{11.44\pm4.15} $` |
					| FiLM-final | 10-W`$_2$` | `$ -10.1\pm11.7 $` | `$ -34.5\pm28.4 $` | `$ \mathbf{-33.9\pm28.6 }$` | `$ -3.50\pm6.79 $` | `$ 0.29\pm9.14 $` | `$ 5.88\pm4.97 $` |
					| Label-multiply | 11-W`$_2$` | <span style="color:gray">`$ -7.2\pm9.8 $`</span> | <span style="color:gray">`$ \mathbf{-15.3\pm15.5 }$`</span> | `$ -18.6\pm20.3 $` | `$ -1.86\pm6.84 $` | `$ \mathbf{2.36\pm8.80} $` | `$ 5.13\pm4.07 $` |
					</textarea>
				</section>

				<section data-markdown  style="font-size: medium">
					<textarea data-template>
					## Visual conditioning results

					|Method | Frames | ID | SI-SDR `$\uparrow$` | SD-SDR `$\uparrow$` | PES `$\downarrow$` | SDR `$\uparrow$` | SIR `$\uparrow$` | SAR `$\uparrow$` |
					| :--- | :---: | :-----: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |
					|w/o conditioning | 0 | 6-W`$_2$` | `$ -7.4\pm 9.9 $` | `$ \mathbf{-17.5\pm 19.8} $` | `$ -24.4\pm 15.8 $` | `$ -2.27\pm6.85 $` | `$ \mathbf{1.46\pm8.39} $` | `$ 5.74\pm5.23 $` |
					|FiLM-encoder | 1 | 12-W`$_2$` | `$ -8.3\pm8.9 $` | `$ -26.1\pm17.5 $` | `$ -12.2\pm13.4 $` | `$ -3.38\pm5.54 $` | `$ -0.69\pm6.60 $` | `$ 5.91\pm4.25 $`  |
					|FiLM-bottleneck | 1 | 17-W`$_2$` | `$ -8.7\pm10.8 $` | `$ -20.7\pm20.1 $` | `$ \mathbf{-25.9\pm23.9} $` | `$ -3.02\pm7.10 $` | <span style="color:gray">`$ 1.17\pm8.95 $`</span> | <span style="color:gray">`$ 5.09\pm5.08 $`</span> |
					|FiLM-final | 1 | 18-W`$_2$` | `$ -4.8\pm7.4 $` | `$ -18.4\pm13.5 $` | `$ -10.9\pm8.7 $` | <span style="color:gray">`$ -2.24\pm6.22 $`</span> | `$ 0.05\pm7.91 $` | `$ 8.87\pm4.90 $` |
					|Final-multiply | 1 | 16-W`$_2$` | `${-4.1\pm7.2}$` | `$ -19.0\pm13.9 $` | `$ -15.4\pm14.3 $` | `$ \mathbf{-1.49\pm6.08} $` | `$ 0.98\pm8.39 $` | `$ 9.18\pm3.71 $` |
					|FiLM-final-lstm | 5 | 13-W`$_2$` | `$ \mathbf{-3.8\pm6.3} $` | `$ -22.7\pm12.6 $` | `$ -9.4\pm12.2 $` | `$ -2.51\pm5.32 $` | `$ -1.43\pm6.38 $` | `$ \mathbf{12.44\pm4.72} $` |
					|FiLM-bottleneck-lstm | 5 | 14-W`$_2$` | `$ -4.7\pm6.9 $` | `$ -17.9\pm10.7 $` | `$ -10.3\pm12.7 $` | `$ -1.55\pm5.94 $` | `$ 1.02\pm7.96 $` | `$ 7.90\pm3.27 $` |
					|FiLM-bottleneck-mp | 5 | 15-W`$_2$` | `$ -7.0\pm7.8 $` | `$ -23.0\pm14.4 $` | `$ -12.1\pm15.0 $` | `$ -2.46\pm5.82 $` | `$ 0.54\pm7.66 $` | `$ 6.28\pm3.61 $` |
					|SoP-unet7 | 3 | 19 | `$-18.7\pm8.9$` | `$-21.1\pm9.4$` | n/a | `$-3.76\pm4.00$` | `$-1.45\pm4.68$` | `$7.56\pm3.13$` |
					|SoP-unet7-ft | 3 | 20 | `$-17.5\pm8.5$` | `$-20.3\pm9.3$` | n/a | `$-2.57\pm4.99$` | `$0.47\pm6.43$` | `$6.89\pm2.48$` |
					|SoP-unet5-Solos | 3 | 21 | `$-16.9\pm8.6$` | `$-18.7\pm8.9$` | n/a | `$-2.9\pm4.7$` | `$-1.67\pm5.34$` | `$11.07\pm6.87$` |
					</textarea>
				</section>

			</section>

			<section class="cover" data-background="img/violin.png" data-state="no-title-footer no-progressbar has-dark-background">
				<h2>IV - Conclusion</h2>
			</section>

			<section data-state="is-in-conclusion">
				<h3>Contributions</h3>
				<blockquote class="fragment" data-fragment-index="1"> modern and efficient audio-visual methods for MIR </blockquote>
				<ul>
					<li class="fragment" data-fragment-index="2"> for MIR
						<span class="fragment" data-fragment-index="3" style="color: #c8102e"> â†’ classification, separation </span> </li>
					<li class="fragment" data-fragment-index="4"> methods
						<span class="fragment" data-fragment-index="5" style="color: #c8102e"> â†’ data fusion, conditioning </span> </li>
					<li class="fragment" data-fragment-index="6"> audio-visual
						<span class="fragment" data-fragment-index="7" style="color: #c8102e"> â†’ datasets (YouTube8m-MusInst, Solos) </span> </li>
					<li class="fragment" data-fragment-index="8">modern and efficient
						<span class="fragment" data-fragment-index="9" style="color: #c8102e"> â†’ deep learning </span> </li>
				</ul>
			</section>
			<section data-state="is-in-conclusion">
				<h3>Research focus</h3>
				<blockquote class="fragment"> Where can we merge different data representations? </blockquote>
				<blockquote class="fragment"> How should we merge data from different sources? </blockquote>

				<!--
					<li class="affirmation fragment"> Late fusion and hybrid fusion for high-level concepts</li>
					<li class="affirmation fragment"> Early fusion for continuous and strong conditioning</li>
					-->
			</section>

			<section data-state="is-in-conclusion">
				<h3>Publications</h3>
				<div class="references">
					<h5 style="text-align: left; margin-bottom: 0em; margin-left: 3em">Audio-Visual MIR</h5>
					<ul style="font-size: smaller">
						<li style="font-weight:bold;">O. Slizovskaia, E. GÃ³mez, and G. Haro. (SMC, 2016) Automatic musical instrument recognition in audiovisual recordings by combining image and audio classification strategies.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/SMC2016</a></span>
						</li>
						<li style="font-weight:bold;">O. Slizovskaia, E. GÃ³mez, and G. Haro. (ACM ICMR, 2017) Musical instrument recognition in user-generated videos using a multimodal convolutional neural network architecture.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/ICMR2017</a></span>
						</li>
						<li style="font-weight:bold;">O. Slizovskaia, E. GÃ³mez, and G. Haro. (ISMIR-LBD, 2017) Correspondence between audio and visual deep models for musical instrument detection in video recordings.</li>
						<li style="font-weight: bold;">O. Slizovskaia, E. GÃ³mez, and G. Haro. (ICML-ML4M, 2018) A Case Study of Deep-Learned Activations via Hand-Crafted Audio Features.</li>
						<li style="font-weight: bold;">O. Slizovskaia, L. Kim, E. GÃ³mez, and G. Haro. (ICASSP, 2019) End-to-end sound source separation conditioned on instrument labels.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/vimss</a></span>
						</li>
						<li>J.F. Montesinos, O. Slizovskaia, and G. Haro. (IEEE MMSP, 2020) Solos: A Dataset for Audio-Visual Music Analysis.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/JuanFMontesinos/Solos</a></span>
						</li>
						<li style="font-weight: bold;">O. Slizovskaia, G. Haro and E. GÃ³mez. (IEEE/ACM TASLP, 2020) Conditioned Source Separation for Musical Instrument Performances (under review after major changes).
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/conditioned-u-net</a></span>
						</li>
					</ul>
					<h5 style="text-align: left; margin-top: 1em; margin-bottom: 0em; margin-left: 3em">Collaborations</h5>
					<ul style="font-size: smaller">
						<li>J. Pons, O. Slizovskaia, R. Gong, E GÃ³mez, X. Serra. (EUSIPCO, 2017) Timbre analysis of music audio signals with convolutional neural networks.
							<br> &emsp; <span style="color: #c8102e"><a>https://github.com/Veleslavia/EUSIPCO2017</a></span>
						</li>
						<li>E. Fonseca, R. Gong, D. Bogdanov, O. Slizovskaia, E. GÃ³mez, X. Serra. (DCASE, 2017) Acoustic scene classification by ensembling gradient boosting machine and convolutional neural networks. </li>
						<li>R. Gong, E. Fonseca, D. Bogdanov, O. Slizovskaia, E. GÃ³mez, X. Serra. (DCASE, 2017) Acoustic scene classification by fusing LightGBM and VGG-net multichannel predictions. </li>
						<li>J. SerrÃ , D. Ãlvarez, V. GÃ³mez, O. Slizovskaia, J.F. NÃºÃ±ez, and J. Luque. (ICLR, 2020) Input complexity and out-of-distribution detection with likelihood-based generative models. </li>
						<li>D. Michelsanti, O. Slizovskaia, G. Haro, E. GÃ³mez, Z.H. Tan, and J. Jensen. (INTERSPEECH, 2020) Vocoder-Based Speech Synthesis from Silent Videos. </li>
					</ul>
				</div>
			</section>

			<section class="cover" data-background="img/mix1.png" data-state="no-title-footer no-progressbar has-dark-background">
			<aside class="notes"> See Integrated Gradients at https://captum.ai/docs/captum_insights</aside>
				Thank you!
			</section>

			<!--
			<section>
				<h3><span style="color: #ff637d">Thank you!</span></h3>
				<br>
				<span style="color: orange">Courtesy </span>
				<br>
				<ul>
					<li> URMP Dataset: <br><a style="font-size: smaller;" href="https://doi.org/10.5061/dryad.ng3r749">https://doi.org/10.5061/dryad.ng3r749</a></li>
					<li> TensorFlow Research Cloud and Jeju DL Camp</li>
					<li> Maria de Maeztu program</li>
					<li> GitHub: <a style="font-size: smaller;" href="https://github.com/Veleslavia/vimss">https://github.com/Veleslavia/vimss</a></li>
				</ul>
			</section>
			-->

			<div class="footer">
				<img height="70px" src="img/upf-logo.png" alt="logo" class="logo" id="upf"/>
				<div id="middlebox" style="color: rgba(0, 0, 0, 0.2)">
					<span id="background">Background </span>|
					<span id="classification"> Classification </span> |
					<span id="separation">Separation </span> |
					<span id="conclusion">Conclusion</span>
				</div>
			</div>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies



		Reveal.initialize({

			math: {
				  mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				  config: 'TeX-AMS_HTML-full',
				  // pass other options into `MathJax.Hub.Config()`
				  TeX: { extensions: ["color.js"], Macros: { RR: "{\\bf R}" }
				  },
			},
			// specified using percentage units.
			//width: 960,
			//height: 700,
			//controls: false,
			progress: true,
			//history: true,
			//center: false,
			slideNumber: true,
			overview: true,
			hash: true,
			help: true,
			//minScale: 0.1,
			//maxScale: 5,
			//transition: 'none', //
			audio: {
				prefix: 'audio/', 	// audio files are stored in the "audio" folder
				suffix: '.wav',		// audio files have the ".wav" ending
				textToSpeechURL: null,  // the URL to the text to speech converter
				defaultNotes: false, 	// use slide notes as default for the text to speech converter
				defaultText: false, 	// use slide text as default for the text to speech converter
				advance: -10, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance 
				autoplay: true,	// automatically start slideshow
				defaultDuration: 10,	// default duration in seconds if no audio is available 
				defaultAudios: false,	// try to play audios with names such as audio/1.2.ogg
			},
			plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealMath],
			dependencies: [
				{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function () { return !!document.body.classList; } },
			]
		});
		</script>
	</body>
</html>
